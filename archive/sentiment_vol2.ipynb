{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Google Play store apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>App</th>\n",
       "      <th>Translated_Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Polarity</th>\n",
       "      <th>Sentiment_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>I like eat delicious food. That's I'm cooking ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>This help eating healthy exercise regular basis</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.288462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Works great especially going grocery store</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Best idea us</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Best way</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Amazing</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Looking forward app,</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>It helpful site ! It help foods get !</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>good you.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Useful information The amount spelling errors ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      App                                  Translated_Review  \\\n",
       "0   10 Best Foods for You  I like eat delicious food. That's I'm cooking ...   \n",
       "1   10 Best Foods for You    This help eating healthy exercise regular basis   \n",
       "3   10 Best Foods for You         Works great especially going grocery store   \n",
       "4   10 Best Foods for You                                       Best idea us   \n",
       "5   10 Best Foods for You                                           Best way   \n",
       "6   10 Best Foods for You                                            Amazing   \n",
       "8   10 Best Foods for You                               Looking forward app,   \n",
       "9   10 Best Foods for You              It helpful site ! It help foods get !   \n",
       "10  10 Best Foods for You                                          good you.   \n",
       "11  10 Best Foods for You  Useful information The amount spelling errors ...   \n",
       "\n",
       "   Sentiment  Sentiment_Polarity  Sentiment_Subjectivity  \n",
       "0   Positive                1.00                0.533333  \n",
       "1   Positive                0.25                0.288462  \n",
       "3   Positive                0.40                0.875000  \n",
       "4   Positive                1.00                0.300000  \n",
       "5   Positive                1.00                0.300000  \n",
       "6   Positive                0.60                0.900000  \n",
       "8    Neutral                0.00                0.000000  \n",
       "9    Neutral                0.00                0.000000  \n",
       "10  Positive                0.70                0.600000  \n",
       "11  Positive                0.20                0.100000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv('googleplaystore_user_reviews.csv')\n",
    "reviews = reviews.dropna()\n",
    "reviews.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data cleaning & wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode sentiment into numeric values\n",
    "conditions = [\n",
    "    (reviews['Sentiment'] == 'Positive'),\n",
    "    (reviews['Sentiment'] == 'Neutral'),\n",
    "    (reviews['Sentiment'] == 'Negative')]\n",
    "\n",
    "choices = [1, 0, -1]\n",
    "reviews['Sentiment_encode'] = np.select(conditions, choices, default= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    23998\n",
       "-1     8271\n",
       " 0     5158\n",
       "Name: Sentiment_encode, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the sentiment distribution\n",
    "reviews.Sentiment_encode.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text data\n",
    "def clean_text(sentence):\n",
    "    sent = sentence.lower()  # lowercase\n",
    "    sent = re.sub(r'[^\\w\\s]',' ',sent) # remove punctuation\n",
    "    sent = sent.replace(os.linesep,\"\")  # remove line break\n",
    "    sent = re.sub(r'\\d+','',sent)  # remove digits\n",
    "#     sent = ' '.join([tok for tok in sent.split() if tok not in STOP_WORDS]) # remove stopwords vs. with stopwords\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>App</th>\n",
       "      <th>Translated_Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Polarity</th>\n",
       "      <th>Sentiment_Subjectivity</th>\n",
       "      <th>Sentiment_encode</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>I like eat delicious food. That's I'm cooking ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1</td>\n",
       "      <td>i like eat delicious food  that s i m cooking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>This help eating healthy exercise regular basis</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.288462</td>\n",
       "      <td>1</td>\n",
       "      <td>this help eating healthy exercise regular basis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Works great especially going grocery store</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1</td>\n",
       "      <td>works great especially going grocery store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Best idea us</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>best idea us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Best way</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>best way</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     App                                  Translated_Review  \\\n",
       "0  10 Best Foods for You  I like eat delicious food. That's I'm cooking ...   \n",
       "1  10 Best Foods for You    This help eating healthy exercise regular basis   \n",
       "3  10 Best Foods for You         Works great especially going grocery store   \n",
       "4  10 Best Foods for You                                       Best idea us   \n",
       "5  10 Best Foods for You                                           Best way   \n",
       "\n",
       "  Sentiment  Sentiment_Polarity  Sentiment_Subjectivity Sentiment_encode  \\\n",
       "0  Positive                1.00                0.533333                1   \n",
       "1  Positive                0.25                0.288462                1   \n",
       "3  Positive                0.40                0.875000                1   \n",
       "4  Positive                1.00                0.300000                1   \n",
       "5  Positive                1.00                0.300000                1   \n",
       "\n",
       "                                             reviews  \n",
       "0  i like eat delicious food  that s i m cooking ...  \n",
       "1    this help eating healthy exercise regular basis  \n",
       "3         works great especially going grocery store  \n",
       "4                                       best idea us  \n",
       "5                                           best way  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['reviews'] = reviews['Translated_Review'].apply(clean_text)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8671     been using paid version years now  originally ...\n",
       "29070    i love app  using ages  however latest ver   s...\n",
       "58115    i hate  weeks waiting items i find not getting...\n",
       "12111                                             the best\n",
       "2609     new tos data collection   i m out     uninstal...\n",
       "Name: reviews, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews['reviews'],reviews['Sentiment_encode'],test_size = 0.3, random_state=0)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>App</th>\n",
       "      <th>Translated_Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Polarity</th>\n",
       "      <th>Sentiment_Subjectivity</th>\n",
       "      <th>Sentiment_encode</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>Apex Launcher</td>\n",
       "      <td>Been using paid version years now. Originally ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>1</td>\n",
       "      <td>been using paid version years now  originally ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29070</th>\n",
       "      <td>ConvertPad - Unit Converter</td>\n",
       "      <td>I love app, using ages, however latest ver 3.1...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>1</td>\n",
       "      <td>i love app  using ages  however latest ver   s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58115</th>\n",
       "      <td>H&amp;M</td>\n",
       "      <td>I hate 2 weeks waiting items I find NOT gettin...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-0.900000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>-1</td>\n",
       "      <td>i hate  weeks waiting items i find not getting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12111</th>\n",
       "      <td>Bagan - Myanmar Keyboard</td>\n",
       "      <td>The best</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>the best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>AC - Tips &amp; News for Android™</td>\n",
       "      <td>New TOS data collection.. I'm out!!! (Uninstal...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.266335</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>1</td>\n",
       "      <td>new tos data collection   i m out     uninstal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 App  \\\n",
       "8671                   Apex Launcher   \n",
       "29070    ConvertPad - Unit Converter   \n",
       "58115                            H&M   \n",
       "12111       Bagan - Myanmar Keyboard   \n",
       "2609   AC - Tips & News for Android™   \n",
       "\n",
       "                                       Translated_Review Sentiment  \\\n",
       "8671   Been using paid version years now. Originally ...  Positive   \n",
       "29070  I love app, using ages, however latest ver 3.1...  Positive   \n",
       "58115  I hate 2 weeks waiting items I find NOT gettin...  Negative   \n",
       "12111                                           The best  Positive   \n",
       "2609   New TOS data collection.. I'm out!!! (Uninstal...  Positive   \n",
       "\n",
       "       Sentiment_Polarity  Sentiment_Subjectivity Sentiment_encode  \\\n",
       "8671             0.340000                0.680000                1   \n",
       "29070            0.425000                0.550000                1   \n",
       "58115           -0.900000                0.950000               -1   \n",
       "12111            1.000000                0.300000                1   \n",
       "2609             0.266335                0.454545                1   \n",
       "\n",
       "                                                 reviews  \n",
       "8671   been using paid version years now  originally ...  \n",
       "29070  i love app  using ages  however latest ver   s...  \n",
       "58115  i hate  weeks waiting items i find not getting...  \n",
       "12111                                           the best  \n",
       "2609   new tos data collection   i m out     uninstal...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(reviews,test_size = 0.3, random_state=0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted values: [-1  0 -1 ...  1 -1  1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8831596758393445"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic regression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "text_clf_LR = Pipeline([('vect', CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', LogisticRegression())])\n",
    "\n",
    "text_clf_LR.fit(train.reviews.values, train.Sentiment_encode.astype('int')) \n",
    "predicted_LR = text_clf_LR.predict(test.reviews.values)\n",
    "\n",
    "print('predicted values:',predicted_LR)\n",
    "accuracy_score(predicted_LR, test.Sentiment_encode.astype('int'))   # logistic regression 有0.87 accuracy\n",
    "# accuracy_score(predicted_LR, X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>N-gram with logistic regression：（-> unigram with stopwords效果最好）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['analyzer', 'binary', 'decode_error', 'dtype', 'encoding', 'input', 'lowercase', 'max_df', 'max_features', 'min_df', 'ngram_range', 'preprocessor', 'stop_words', 'strip_accents', 'token_pattern', 'tokenizer', 'vocabulary'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "CountVectorizer().get_params().keys() # check the available params of CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted values: [-1  0 -1 ...  1 -1  1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8859203847181405"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare accuracy of unigram, bigram, trigram\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "lr = LogisticRegression()\n",
    "n_features = np.arange(10000,100001,10000)  # 这里我只取了10000作为max_features\n",
    "\n",
    "# cvec.set_params(stop_words = STOP_WORDS, max_features=10000, ngram_range=(1,1))    # unigram without stopwords: 0.8721\n",
    "# cvec.set_params(max_features=10000, ngram_range=(1,2))  # bigram without stopwords: 0.87024\n",
    "# cvec.set_params(max_features=10000, ngram_range=(1,3))  # trigram without stopwords: 0.8705\n",
    "\n",
    "text_clf_LR = Pipeline([('vect', cvec),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', LogisticRegression())])\n",
    "\n",
    "text_clf_LR.fit(train.reviews.values, train.Sentiment_encode.astype('int')) \n",
    "predicted_LR = text_clf_LR.predict(test.reviews.values)\n",
    "print('predicted values:',predicted_LR)\n",
    "accuracy_score(predicted_LR, test.Sentiment_encode.astype('int'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer()的params: 排除了stopwords之后，unigram的表现最好，好于bigram和trigram\n",
    "- 有stopwords跟排除stopwords结果差不多；\n",
    "- max_features增加，accuracy反而下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO： plot出unigram, bigram, trigram比较图(iterate over num. of features)\n",
    "# 参考：https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-4-count-vectorizer-b3f4944e51b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TfidfVectorizer() vs. CountVectorizer() (-> 没有显著差别）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['analyzer', 'binary', 'decode_error', 'dtype', 'encoding', 'input', 'lowercase', 'max_df', 'max_features', 'min_df', 'ngram_range', 'norm', 'preprocessor', 'smooth_idf', 'stop_words', 'strip_accents', 'sublinear_tf', 'token_pattern', 'tokenizer', 'use_idf', 'vocabulary'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TfidfVectorizer().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted values: [ 0 -1 -1 ...  1 -1  1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8721168403241607"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "lr = LogisticRegression()\n",
    "\n",
    "tvec.set_params(stop_words = STOP_WORDS, max_features=10000, ngram_range=(1,1))  # unigram  -> o.8721\n",
    "# tvec.set_params(stop_words = STOP_WORDS, max_features=10000, ngram_range=(1,2))  # bigram  -> 0.87024\n",
    "\n",
    "text_clf_LR_tfidf = Pipeline([('vect', tvec),\n",
    "                         ('clf', LogisticRegression())])\n",
    "\n",
    "text_clf_LR_tfidf.fit(train.reviews.values, train.Sentiment_encode.astype('int')) \n",
    "predicted_LR = text_clf_LR_tfidf.predict(test.reviews.values)\n",
    "print('predicted values:',predicted_LR)\n",
    "accuracy_score(predicted_LR, test.Sentiment_encode.astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前用CountVectorizer()是pipeline包含了tf-idf transformer的，所以performance跟使用tdidfVectorizer()效果一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Other Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted values: [ 0 -1 -1 ...  1 -1  1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9244812538961618"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "svc = LinearSVC()\n",
    "\n",
    "# tvec.set_params(stop_words = STOP_WORDS, max_features=10000, ngram_range=(1,1))  # unigram -> 0.893\n",
    "tvec.set_params(max_features=10000, ngram_range=(1,3))  # trigram -> 0.8941 accuracy\n",
    "\n",
    "text_clf_svc_tfidf = Pipeline([('vect', tvec),\n",
    "                         ('clf', LinearSVC())])\n",
    "\n",
    "text_clf_svc_tfidf.fit(train.reviews.values, train.Sentiment_encode.astype('int')) \n",
    "predicted_svc = text_clf_svc_tfidf.predict(test.reviews.values)\n",
    "print('predicted values:',predicted_svc)\n",
    "accuracy_score(predicted_svc, test.Sentiment_encode.astype('int'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted values: [ 0  1 -1 ...  1  1  1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8773710927063852"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RidgeClassifier()\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "rc = RidgeClassifier()\n",
    "\n",
    "# tvec.set_params(stop_words = STOP_WORDS, max_features=10000, ngram_range=(1,1))   # unigram: 0.844\n",
    "tvec.set_params(max_features=10000, ngram_range=(1,2)) # bigram: 0.856\n",
    "\n",
    "text_clf_rc_tfidf = Pipeline([('vect', tvec),\n",
    "                         ('clf', RidgeClassifier())])\n",
    "\n",
    "text_clf_rc_tfidf.fit(train.reviews.values, train.Sentiment_encode.astype('int')) \n",
    "predicted_rc = text_clf_rc_tfidf.predict(test.reviews.values)\n",
    "print('predicted values:',predicted_rc)\n",
    "accuracy_score(predicted_rc, test.Sentiment_encode.astype('int'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted values: [ 0 -1 -1 ...  1 -1  1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9279544037759373"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PassiveAggressiveClassifier()\n",
    "\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "pac = PassiveAggressiveClassifier()\n",
    "\n",
    "# tvec.set_params(stop_words = STOP_WORDS, max_features=10000, ngram_range=(1,1))  # unigram: 0.893\n",
    "tvec.set_params(max_features=10000, ngram_range=(1,2))    # bigram: 0.895 (max)\n",
    "\n",
    "text_clf_pac_tfidf = Pipeline([('vect', tvec),\n",
    "                         ('clf', PassiveAggressiveClassifier())])\n",
    "\n",
    "text_clf_pac_tfidf.fit(train.reviews.values, train.Sentiment_encode.astype('int')) \n",
    "predicted_pac = text_clf_pac_tfidf.predict(test.reviews.values)\n",
    "print('predicted values:',predicted_pac)\n",
    "accuracy_score(predicted_pac, test.Sentiment_encode.astype('int'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>结果:</h4> \n",
    "linear_SVC(trigram:0.894) | PassiveAggressiveClassifier(bigram:0.895) | Logistic_regression (unigram: 0.87021) | Ridge Classifier (bigram: 0.856) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Ensemble classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结合上面几个performance比较好的classifier,去建一个ensemble classifier。再看performance是否变好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此function用于比较pipeline classifier, 基于accuracy和training time (TODO：内部结构待弄懂)\n",
    "\n",
    "from time import time\n",
    "\n",
    "def accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n",
    "    t0 = time()\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print (\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "    print (\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    if accuracy > null_accuracy:\n",
    "        print (\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "    elif accuracy == null_accuracy:\n",
    "        print (\"model has the same accuracy with the null accuracy\")\n",
    "    else:\n",
    "        print (\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "    print (\"train and test time: {0:.2f}s\".format(train_test_time))\n",
    "    print (\"-\"*80)\n",
    "    return accuracy, train_test_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result for Logistic Regression\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 87.02%\n",
      "model is 0.81% more accurate than null accuracy\n",
      "train and test time: 3.50s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Linear SVC\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 89.34%\n",
      "model is 3.13% more accurate than null accuracy\n",
      "train and test time: 2.58s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for LinearSVC with L1-based feature selection\n",
      "Pipeline(memory=None,\n",
      "     steps=[('feature_selection', SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
      "     verbose=0),\n",
      "        max_features=None, n...ax_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0))])\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 89.39%\n",
      "model is 3.18% more accurate than null accuracy\n",
      "train and test time: 3.96s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Multinomial NB\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 73.57%\n",
      "model is 12.65% less accurate than null accuracy\n",
      "train and test time: 2.48s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Ridge Classifier\n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=None, solver='auto',\n",
      "        tol=0.001)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 85.68%\n",
      "model is 0.53% less accurate than null accuracy\n",
      "train and test time: 3.36s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Passive-Aggresive\n",
      "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
      "              early_stopping=False, fit_intercept=True, loss='hinge',\n",
      "              max_iter=None, n_iter=None, n_iter_no_change=5, n_jobs=None,\n",
      "              random_state=None, shuffle=True, tol=None,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 89.30%\n",
      "model is 3.09% more accurate than null accuracy\n",
      "train and test time: 2.39s\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Logistic Regression', 0.8702466826965892, 3.498500108718872),\n",
       " ('Linear SVC', 0.8934010152284264, 2.5845491886138916),\n",
       " ('LinearSVC with L1-based feature selection',\n",
       "  0.8939353459791611,\n",
       "  3.957974910736084),\n",
       " ('Multinomial NB', 0.735684388636566, 2.4809072017669678),\n",
       " ('Ridge Classifier', 0.8567993588030991, 3.3647420406341553),\n",
       " ('Passive-Aggresive', 0.8930447947279366, 2.3886570930480957)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare the accuracy and training time for each classifier\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "names = [\"Logistic Regression\", \"Linear SVC\", \"LinearSVC with L1-based feature selection\",\"Multinomial NB\", \n",
    "         \"Ridge Classifier\", \"Passive-Aggresive\"]\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    LinearSVC(),\n",
    "    Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False))),\n",
    "  ('classification', LinearSVC(penalty=\"l2\"))]),\n",
    "    MultinomialNB(),\n",
    "    RidgeClassifier(),\n",
    "    PassiveAggressiveClassifier()\n",
    "    ]\n",
    "\n",
    "zipped_clf = zip(names,classifiers)\n",
    "tvec = TfidfVectorizer()\n",
    "\n",
    "def classifier_comparator(vectorizer=tvec, n_features=10000, stop_words=None, ngram_range=(1, 1), classifier=zipped_clf):\n",
    "    result = []\n",
    "    vectorizer.set_params(stop_words=STOP_WORDS, max_features=n_features, ngram_range=ngram_range)\n",
    "    for n,c in classifier:\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', c)\n",
    "        ])\n",
    "        print (\"Validation result for {}\".format(n))\n",
    "        print (c)\n",
    "#         clf_accuracy,tt_time = accuracy_summary(checker_pipeline, x_train, y_train, x_validation, y_validation)\n",
    "        clf_accuracy,tt_time = accuracy_summary(checker_pipeline,train.reviews.values, train.Sentiment_encode.astype('int'), test.reviews.values, test.Sentiment_encode.astype('int'))\n",
    "        result.append((n,clf_accuracy,tt_time))\n",
    "    return result\n",
    "\n",
    "bigram_result = classifier_comparator(n_features=10000,ngram_range=(1,2))\n",
    "bigram_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result for Logistic Regression\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 88.90%\n",
      "model is 2.69% more accurate than null accuracy\n",
      "train and test time: 3.38s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Linear SVC\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 92.51%\n",
      "model is 6.30% more accurate than null accuracy\n",
      "train and test time: 3.22s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Multinomial NB\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 74.01%\n",
      "model is 12.20% less accurate than null accuracy\n",
      "train and test time: 2.87s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Ridge Classifier\n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=None, solver='auto',\n",
      "        tol=0.001)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 87.74%\n",
      "model is 1.52% more accurate than null accuracy\n",
      "train and test time: 3.75s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Passive Aggresive Classifier\n",
      "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
      "              early_stopping=False, fit_intercept=True, loss='hinge',\n",
      "              max_iter=None, n_iter=None, n_iter_no_change=5, n_jobs=None,\n",
      "              random_state=None, shuffle=True, tol=None,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 92.54%\n",
      "model is 6.32% more accurate than null accuracy\n",
      "train and test time: 2.88s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Ensemble\n",
      "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)), ('svc', LinearS...=None, shuffle=True, tol=None,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False))],\n",
      "         flatten_transform=None, n_jobs=None, voting='hard', weights=None)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 90.71%\n",
      "model is 4.50% more accurate than null accuracy\n",
      "train and test time: 5.04s\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# compare results of each classifier and emsemble classifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = LinearSVC()\n",
    "clf3 = MultinomialNB()\n",
    "clf4 = RidgeClassifier()\n",
    "clf5 = PassiveAggressiveClassifier()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('svc', clf2), ('mnb', clf3), ('rcs', clf4), ('pac', clf5)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, eclf], ['Logistic Regression', 'Linear SVC', 'Multinomial NB', 'Ridge Classifier', 'Passive Aggresive Classifier', 'Ensemble']):\n",
    "    checker_pipeline = Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer(max_features=10000,ngram_range=(1, 2))),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "    print (\"Validation result for {}\".format(label))\n",
    "    print (clf)\n",
    "#     clf_accuracy,tt_time = accuracy_summary(checker_pipeline, x_train, y_train, x_validation, y_validation)\n",
    "    clf_accuracy,tt_time = accuracy_summary(checker_pipeline,train.reviews.values, train.Sentiment_encode.astype('int'), test.reviews.values, test.Sentiment_encode.astype('int'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论：LinearSVC and Passive Aggresive classifier yield the best performance ~92% accuracy, even better than the voting classifier(emsemble) of 90% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*P.s: 在此project, 将tutorial中的x_train 改为train.reviews.values，y_train改为 train.Sentiment_encode.astype('int') 即可；<br>\n",
    "test data同理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute positive, negative proportion for each word (tutorial Part 5)\n",
    "# 目的是用于lexical approach for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledSentence(words=['i', 'like', 'eat', 'delicious', 'food', 'that', 's', 'i', 'm', 'cooking', 'food', 'myself', 'case', 'best', 'foods', 'helps', 'lot', 'also', 'best', 'before', 'shelf', 'life'], tags=['all_0']),\n",
       " LabeledSentence(words=['this', 'help', 'eating', 'healthy', 'exercise', 'regular', 'basis'], tags=['all_1']),\n",
       " LabeledSentence(words=['works', 'great', 'especially', 'going', 'grocery', 'store'], tags=['all_3']),\n",
       " LabeledSentence(words=['best', 'idea', 'us'], tags=['all_4']),\n",
       " LabeledSentence(words=['best', 'way'], tags=['all_5'])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "def labelize_tweets_ug(reviews,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, r in zip(reviews.index, reviews):\n",
    "        result.append(LabeledSentence(r.split(), [prefix + '_%s' % i]))\n",
    "    return result\n",
    "\n",
    "all_x_w2v = labelize_tweets_ug(reviews.reviews, 'all')\n",
    "all_x_w2v[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 1. DBOW (Distributed Bag Of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37427/37427 [00:00<00:00, 1748674.02it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2215795.05it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2381916.63it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2401925.08it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1953389.19it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2393355.94it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1965495.77it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2049216.31it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2429996.68it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2432368.77it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2099227.28it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2412259.75it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2207289.41it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2417311.61it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2134420.38it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2432218.03it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2374063.73it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2455539.99it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2165274.22it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2439171.75it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2151710.84it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2056329.79it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2431577.56it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2433009.65it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2198109.89it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2211861.24it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2272572.47it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2452279.44it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2269254.46it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2418726.94it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2427328.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7235728916199127"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm  # Instantly make your loops show a smart progress meter\n",
    "from sklearn import utils\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "model_ug_dbow = Doc2Vec(dm=0, size=100, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dbow.build_vocab([x for x in tqdm(all_x_w2v)])   \n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dbow.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dbow.alpha -= 0.002\n",
    "    model_ug_dbow.min_alpha = model_ug_dbow.alpha\n",
    "    \n",
    "\n",
    "def get_vectors(model, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = model.docvecs[prefix]\n",
    "        n += 1\n",
    "    return vecs\n",
    "\n",
    "train_vecs_dbow = get_vectors(model_ug_dbow, train.reviews, 100)\n",
    "validation_vecs_dbow = get_vectors(model_ug_dbow, test.reviews, 100)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "# clf.fit(train_vecs_dbow, y_train)\n",
    "clf.fit(train_vecs_dbow, train.Sentiment_encode.astype('int'))\n",
    "# clf.score(validation_vecs_dbow, y_validation)\n",
    "clf.score(validation_vecs_dbow, test.Sentiment_encode.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar words of \"facebook\":\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('awfull', 0.3810510039329529),\n",
       " ('selves', 0.3671724498271942),\n",
       " ('international', 0.3513246774673462),\n",
       " ('rookies', 0.34875696897506714),\n",
       " ('planned', 0.3481442928314209),\n",
       " ('intelligent', 0.34326136112213135),\n",
       " ('hd', 0.332422137260437),\n",
       " ('detracting', 0.32369160652160645),\n",
       " ('density', 0.3170939087867737),\n",
       " ('mark', 0.31594252586364746)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('similar words of \"facebook\":')\n",
    "model_ug_dbow.most_similar('facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.DMC (Distributed Memory Concatenation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37427/37427 [00:00<00:00, 1640953.92it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2051036.96it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2164050.40it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2172977.16it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2192154.95it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1819237.86it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2173970.22it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2218927.09it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1977653.68it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2375105.39it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1649402.31it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2412296.82it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1751464.00it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2197063.90it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2249741.55it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2178888.71it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2263789.45it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2254491.11it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2225975.10it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2240749.90it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2183829.50it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2213014.95it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2446203.48it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2280992.95it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2044758.71it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1552200.21it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2011999.38it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2046091.29it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1926137.62it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2075250.06it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1107866.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6631044616617686"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_dmc = Doc2Vec(dm=1, dm_concat=1, size=100, window=2, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dmc.build_vocab([x for x in tqdm(all_x_w2v)])  # 唯一差别在 dm_concat\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dmc.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dmc.alpha -= 0.002\n",
    "    model_ug_dmc.min_alpha = model_ug_dmc.alpha\n",
    "    \n",
    "train_vecs_dmc = get_vectors(model_ug_dmc, train.reviews, 100)\n",
    "validation_vecs_dmc = get_vectors(model_ug_dmc, test.reviews, 100)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmc, train.Sentiment_encode.astype('int'))\n",
    "clf.score(validation_vecs_dmc, test.Sentiment_encode.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar words of \"bad\":\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('fb', 0.4526189863681793),\n",
       " ('whatsapp', 0.4396704435348511),\n",
       " ('link', 0.4389076828956604),\n",
       " ('login', 0.4282771348953247),\n",
       " ('account', 0.402019202709198),\n",
       " ('food', 0.3948511481285095),\n",
       " ('logged', 0.3929815888404846),\n",
       " ('refreshed', 0.385400652885437),\n",
       " ('firefox', 0.3827478885650635),\n",
       " ('photos', 0.3684810996055603)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('similar words of \"bad\":')\n",
    "model_ug_dmc.most_similar('facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('start', 0.40255799889564514),\n",
       " ('difficulty', 0.39897745847702026),\n",
       " ('monitor', 0.3782039284706116),\n",
       " ('certain', 0.3774377405643463),\n",
       " ('turn', 0.3749348223209381),\n",
       " ('border', 0.3658297657966614),\n",
       " ('access', 0.3624209761619568),\n",
       " ('throw', 0.36150044202804565),\n",
       " ('edges', 0.35626035928726196),\n",
       " ('distances', 0.3547050356864929)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ug_dmc.most_similar(positive=['bigger','small'],negative=['big'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3. DMM (Distributed Memory Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37427/37427 [00:00<00:00, 1600891.47it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2240685.93it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1679705.27it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2206265.68it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2134652.58it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1938506.00it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2372987.10it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2111907.76it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2258123.30it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1945858.84it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2289509.46it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2391970.13it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1616935.84it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2089475.65it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1935709.28it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1887030.93it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1981197.90it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2255430.47it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2216984.18it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1928338.05it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2242574.51it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1921987.07it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2216045.28it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2123046.97it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2230307.82it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2459348.52it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2282850.52it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2143601.37it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1994083.25it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2087142.06it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2254782.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6948971413304835"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_dmm = Doc2Vec(dm=1, dm_mean=1, size=100, window=4, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dmm.build_vocab([x for x in tqdm(all_x_w2v)])  #主要差别在 dm_mean\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dmm.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dmm.alpha -= 0.002\n",
    "    model_ug_dmm.min_alpha = model_ug_dmm.alpha\n",
    "    \n",
    "train_vecs_dmm = get_vectors(model_ug_dmm, train.reviews, 100)\n",
    "validation_vecs_dmm = get_vectors(model_ug_dmm, test.reviews, 100)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmm, train.Sentiment_encode.astype('int'))\n",
    "clf.score(validation_vecs_dmm, test.Sentiment_encode.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar words of \"bad\":\n",
      "[('good', 0.5267852544784546), ('poor', 0.4773084223270416), ('disappointed', 0.4378770589828491), ('annoying', 0.4193805158138275), ('addictive', 0.41082853078842163), ('cool', 0.40678340196609497), ('terrible', 0.40267348289489746), ('addicting', 0.4022219181060791), ('unappealing', 0.3983428478240967), ('do', 0.3971942067146301)]\n",
      "------------------------\n",
      "similar words of \"good\":\n",
      "[('great', 0.7731646299362183), ('awesome', 0.6432011723518372), ('nice', 0.6063817143440247), ('amazing', 0.5626569986343384), ('better', 0.539897620677948), ('bad', 0.5267852544784546), ('simple', 0.4833025336265564), ('wonderful', 0.4758867919445038), ('interesting', 0.4753531217575073), ('cool', 0.47357019782066345)]\n"
     ]
    }
   ],
   "source": [
    "print('similar words of \"bad\":')\n",
    "print(model_ug_dmm.most_similar('bad'))\n",
    "print('------------------------')\n",
    "print('similar words of \"good\":')\n",
    "print(model_ug_dmm.most_similar('good'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析：the system successfully capture similar words of \"good\", yet fails to capture similar words of \"bad\" and it recognizes \"good\" as a similar wods of \"bad\"m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4. Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.753851634161546"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DBOW + DMC\n",
    "def get_concat_vectors(model1,model2, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = np.append(model1.docvecs[prefix],model2.docvecs[prefix])\n",
    "        n += 1\n",
    "    return vecs\n",
    "\n",
    "train_vecs_dbow_dmc = get_concat_vectors(model_ug_dbow,model_ug_dmc, train.reviews, 200)\n",
    "validation_vecs_dbow_dmc = get_concat_vectors(model_ug_dbow,model_ug_dmc, test.reviews, 200)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmc, train.Sentiment_encode.astype('int'))\n",
    "clf.score(validation_vecs_dbow_dmc, test.Sentiment_encode.astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析：Combined method has better performance than DMC: 0.663 and DBOW: 0.723 separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7505565945320153"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DBOW and DMM\n",
    "train_vecs_dbow_dmm = get_concat_vectors(model_ug_dbow,model_ug_dmm, train.reviews, 200)\n",
    "validation_vecs_dbow_dmm = get_concat_vectors(model_ug_dbow,model_ug_dmm, test.reviews, 200)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmm, train.Sentiment_encode.astype('int'))\n",
    "clf.score(validation_vecs_dbow_dmm, test.Sentiment_encode.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: phrase modeling using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deeping Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Representation: Sequence Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "## Tokenize the sentences\n",
    "\n",
    "max_features = 276    # max length of reviews\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "# tokenizer.fit_on_texts(list(train_X)+list(test_X))\n",
    "tokenizer.fit_on_texts(list(X_train)+list(X_test))\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad Sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 276\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Embedding Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will be using GLoVE Word2Vec embeddings to explain the enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load glove index\n",
    "\n",
    "def load_glove_index():\n",
    "#     EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    EMBEDDING_FILE = './input/glove.42B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]    # 选最常出现的300 words\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    return embeddings_index\n",
    "\n",
    "glove_embedding_index = load_glove_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(glove_embedding_index.items())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create glove (add polarity and lowercase as well)\n",
    "\n",
    "def create_glove(word_index,embeddings_index):\n",
    "    emb_mean,emb_std = -0.005838499,0.48782197\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    embed_size = all_embs.shape[1]\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size+4))\n",
    "    \n",
    "    count_found = nb_words\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        word_sent = TextBlob(word).sentiment\n",
    "        # Extra information we are passing to our embeddings\n",
    "        extra_embed = [word_sent.polarity,word_sent.subjectivity]\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] =  np.append(embedding_vector,extra_embed)\n",
    "        else:\n",
    "            if word.islower():\n",
    "                embedding_vector = embeddings_index.get(word.capitalize())\n",
    "                if embedding_vector is not None: \n",
    "                    embedding_matrix[i] = np.append(embedding_vector,extra_embed)\n",
    "                else:\n",
    "                    embedding_matrix[i,300:] = extra_embed\n",
    "                    count_found-=1\n",
    "            else:\n",
    "                embedding_matrix[i,300:] = extra_embed\n",
    "                count_found-=1\n",
    "    print(\"Got embedding for \",count_found,\" words.\")\n",
    "    return embedding_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
