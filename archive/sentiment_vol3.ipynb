{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Google Play store apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>App</th>\n",
       "      <th>Translated_Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Polarity</th>\n",
       "      <th>Sentiment_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>I like eat delicious food. That's I'm cooking ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>This help eating healthy exercise regular basis</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.288462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Works great especially going grocery store</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Best idea us</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Best way</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Amazing</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Looking forward app,</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>It helpful site ! It help foods get !</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>good you.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Useful information The amount spelling errors ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      App                                  Translated_Review  \\\n",
       "0   10 Best Foods for You  I like eat delicious food. That's I'm cooking ...   \n",
       "1   10 Best Foods for You    This help eating healthy exercise regular basis   \n",
       "3   10 Best Foods for You         Works great especially going grocery store   \n",
       "4   10 Best Foods for You                                       Best idea us   \n",
       "5   10 Best Foods for You                                           Best way   \n",
       "6   10 Best Foods for You                                            Amazing   \n",
       "8   10 Best Foods for You                               Looking forward app,   \n",
       "9   10 Best Foods for You              It helpful site ! It help foods get !   \n",
       "10  10 Best Foods for You                                          good you.   \n",
       "11  10 Best Foods for You  Useful information The amount spelling errors ...   \n",
       "\n",
       "   Sentiment  Sentiment_Polarity  Sentiment_Subjectivity  \n",
       "0   Positive                1.00                0.533333  \n",
       "1   Positive                0.25                0.288462  \n",
       "3   Positive                0.40                0.875000  \n",
       "4   Positive                1.00                0.300000  \n",
       "5   Positive                1.00                0.300000  \n",
       "6   Positive                0.60                0.900000  \n",
       "8    Neutral                0.00                0.000000  \n",
       "9    Neutral                0.00                0.000000  \n",
       "10  Positive                0.70                0.600000  \n",
       "11  Positive                0.20                0.100000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv('googleplaystore_user_reviews.csv')\n",
    "reviews = reviews.dropna()\n",
    "reviews.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data cleaning & wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode sentiment into numeric values\n",
    "conditions = [\n",
    "    (reviews['Sentiment'] == 'Positive'),\n",
    "    (reviews['Sentiment'] == 'Neutral'),\n",
    "    (reviews['Sentiment'] == 'Negative')]\n",
    "\n",
    "choices = [1, 0, -1]\n",
    "reviews['Sentiment_encode'] = np.select(conditions, choices, default= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    23998\n",
       "-1     8271\n",
       " 0     5158\n",
       "Name: Sentiment_encode, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the sentiment distribution\n",
    "reviews.Sentiment_encode.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text data\n",
    "def clean_text(sentence):\n",
    "    sent = sentence.lower()  # lowercase\n",
    "    sent = re.sub(r'[^\\w\\s]',' ',sent) # remove punctuation\n",
    "    sent = sent.replace(os.linesep,\"\")  # remove line break\n",
    "    sent = re.sub(r'\\d+','',sent)  # remove digits\n",
    "#     sent = ' '.join([tok for tok in sent.split() if tok not in STOP_WORDS]) # remove stopwords vs. with stopwords\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>App</th>\n",
       "      <th>Translated_Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Polarity</th>\n",
       "      <th>Sentiment_Subjectivity</th>\n",
       "      <th>Sentiment_encode</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>I like eat delicious food. That's I'm cooking ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1</td>\n",
       "      <td>i like eat delicious food  that s i m cooking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>This help eating healthy exercise regular basis</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.288462</td>\n",
       "      <td>1</td>\n",
       "      <td>this help eating healthy exercise regular basis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Works great especially going grocery store</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1</td>\n",
       "      <td>works great especially going grocery store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Best idea us</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>best idea us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Best way</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>best way</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     App                                  Translated_Review  \\\n",
       "0  10 Best Foods for You  I like eat delicious food. That's I'm cooking ...   \n",
       "1  10 Best Foods for You    This help eating healthy exercise regular basis   \n",
       "3  10 Best Foods for You         Works great especially going grocery store   \n",
       "4  10 Best Foods for You                                       Best idea us   \n",
       "5  10 Best Foods for You                                           Best way   \n",
       "\n",
       "  Sentiment  Sentiment_Polarity  Sentiment_Subjectivity Sentiment_encode  \\\n",
       "0  Positive                1.00                0.533333                1   \n",
       "1  Positive                0.25                0.288462                1   \n",
       "3  Positive                0.40                0.875000                1   \n",
       "4  Positive                1.00                0.300000                1   \n",
       "5  Positive                1.00                0.300000                1   \n",
       "\n",
       "                                             reviews  \n",
       "0  i like eat delicious food  that s i m cooking ...  \n",
       "1    this help eating healthy exercise regular basis  \n",
       "3         works great especially going grocery store  \n",
       "4                                       best idea us  \n",
       "5                                           best way  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['reviews'] = reviews['Translated_Review'].apply(clean_text)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8671     been using paid version years now  originally ...\n",
       "29070    i love app  using ages  however latest ver   s...\n",
       "58115    i hate  weeks waiting items i find not getting...\n",
       "12111                                             the best\n",
       "2609     new tos data collection   i m out     uninstal...\n",
       "Name: reviews, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews['reviews'],reviews['Sentiment_encode'],test_size = 0.3, random_state=0)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>App</th>\n",
       "      <th>Translated_Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Polarity</th>\n",
       "      <th>Sentiment_Subjectivity</th>\n",
       "      <th>Sentiment_encode</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>Apex Launcher</td>\n",
       "      <td>Been using paid version years now. Originally ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>1</td>\n",
       "      <td>been using paid version years now  originally ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29070</th>\n",
       "      <td>ConvertPad - Unit Converter</td>\n",
       "      <td>I love app, using ages, however latest ver 3.1...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>1</td>\n",
       "      <td>i love app  using ages  however latest ver   s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58115</th>\n",
       "      <td>H&amp;M</td>\n",
       "      <td>I hate 2 weeks waiting items I find NOT gettin...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-0.900000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>-1</td>\n",
       "      <td>i hate  weeks waiting items i find not getting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12111</th>\n",
       "      <td>Bagan - Myanmar Keyboard</td>\n",
       "      <td>The best</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>the best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>AC - Tips &amp; News for Android™</td>\n",
       "      <td>New TOS data collection.. I'm out!!! (Uninstal...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.266335</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>1</td>\n",
       "      <td>new tos data collection   i m out     uninstal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 App  \\\n",
       "8671                   Apex Launcher   \n",
       "29070    ConvertPad - Unit Converter   \n",
       "58115                            H&M   \n",
       "12111       Bagan - Myanmar Keyboard   \n",
       "2609   AC - Tips & News for Android™   \n",
       "\n",
       "                                       Translated_Review Sentiment  \\\n",
       "8671   Been using paid version years now. Originally ...  Positive   \n",
       "29070  I love app, using ages, however latest ver 3.1...  Positive   \n",
       "58115  I hate 2 weeks waiting items I find NOT gettin...  Negative   \n",
       "12111                                           The best  Positive   \n",
       "2609   New TOS data collection.. I'm out!!! (Uninstal...  Positive   \n",
       "\n",
       "       Sentiment_Polarity  Sentiment_Subjectivity Sentiment_encode  \\\n",
       "8671             0.340000                0.680000                1   \n",
       "29070            0.425000                0.550000                1   \n",
       "58115           -0.900000                0.950000               -1   \n",
       "12111            1.000000                0.300000                1   \n",
       "2609             0.266335                0.454545                1   \n",
       "\n",
       "                                                 reviews  \n",
       "8671   been using paid version years now  originally ...  \n",
       "29070  i love app  using ages  however latest ver   s...  \n",
       "58115  i hate  weeks waiting items i find not getting...  \n",
       "12111                                           the best  \n",
       "2609   new tos data collection   i m out     uninstal...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(reviews,test_size = 0.3, random_state=0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted values: [-1  0 -1 ...  1 -1  1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8831596758393445"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic regression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "text_clf_LR = Pipeline([('vect', CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', LogisticRegression())])\n",
    "\n",
    "text_clf_LR.fit(train.reviews.values, train.Sentiment_encode.astype('int')) \n",
    "predicted_LR = text_clf_LR.predict(test.reviews.values)\n",
    "\n",
    "print('predicted values:',predicted_LR)\n",
    "accuracy_score(predicted_LR, test.Sentiment_encode.astype('int'))   # logistic regression 有0.87 accuracy\n",
    "# accuracy_score(predicted_LR, X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>N-gram with logistic regression：（-> unigram with stopwords效果最好）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['analyzer', 'binary', 'decode_error', 'dtype', 'encoding', 'input', 'lowercase', 'max_df', 'max_features', 'min_df', 'ngram_range', 'preprocessor', 'stop_words', 'strip_accents', 'token_pattern', 'tokenizer', 'vocabulary'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "CountVectorizer().get_params().keys() # check the available params of CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted values: [-1  0 -1 ...  1 -1  1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8859203847181405"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare accuracy of unigram, bigram, trigram\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "lr = LogisticRegression()\n",
    "n_features = np.arange(10000,100001,10000)  # 这里我只取了10000作为max_features\n",
    "\n",
    "# cvec.set_params(stop_words = STOP_WORDS, max_features=10000, ngram_range=(1,1))    # unigram without stopwords: 0.8721\n",
    "# cvec.set_params(max_features=10000, ngram_range=(1,2))  # bigram without stopwords: 0.87024\n",
    "# cvec.set_params(max_features=10000, ngram_range=(1,3))  # trigram without stopwords: 0.8705\n",
    "\n",
    "text_clf_LR = Pipeline([('vect', cvec),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', LogisticRegression())])\n",
    "\n",
    "text_clf_LR.fit(train.reviews.values, train.Sentiment_encode.astype('int')) \n",
    "predicted_LR = text_clf_LR.predict(test.reviews.values)\n",
    "print('predicted values:',predicted_LR)\n",
    "accuracy_score(predicted_LR, test.Sentiment_encode.astype('int'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17616"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of features of Count vectorizer\n",
    "len(cvec.get_feature_names())   # 总共有17616个features在count vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer()的params: 排除了stopwords之后，unigram的表现最好，好于bigram和trigram\n",
    "- 有stopwords跟排除stopwords结果差不多；\n",
    "- max_features增加，accuracy反而下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO： plot出unigram, bigram, trigram比较图(iterate over num. of features)\n",
    "# 参考：https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-4-count-vectorizer-b3f4944e51b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TfidfVectorizer() vs. CountVectorizer() (-> 没有显著差别）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['analyzer', 'binary', 'decode_error', 'dtype', 'encoding', 'input', 'lowercase', 'max_df', 'max_features', 'min_df', 'ngram_range', 'norm', 'preprocessor', 'smooth_idf', 'stop_words', 'strip_accents', 'sublinear_tf', 'token_pattern', 'tokenizer', 'use_idf', 'vocabulary'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TfidfVectorizer().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted values: [ 0 -1 -1 ...  1 -1  1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8721168403241607"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "lr = LogisticRegression()\n",
    "\n",
    "tvec.set_params(stop_words = STOP_WORDS, max_features=10000, ngram_range=(1,1))  # unigram  -> o.8721\n",
    "# tvec.set_params(stop_words = STOP_WORDS, max_features=10000, ngram_range=(1,2))  # bigram  -> 0.87024\n",
    "\n",
    "text_clf_LR_tfidf = Pipeline([('vect', tvec),\n",
    "                         ('clf', LogisticRegression())])\n",
    "\n",
    "text_clf_LR_tfidf.fit(train.reviews.values, train.Sentiment_encode.astype('int')) \n",
    "predicted_LR = text_clf_LR_tfidf.predict(test.reviews.values)\n",
    "print('predicted values:',predicted_LR)\n",
    "accuracy_score(predicted_LR, test.Sentiment_encode.astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前用CountVectorizer()是pipeline包含了tf-idf transformer的，所以performance跟使用tdidfVectorizer()效果一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of features of tfidf vectorizer\n",
    "len(tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个tfidf vectorizer一共有10000 features (之前设param时定义10000 作为 max_feature value）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT FOR TRIGRAM WITH STOP WORDS (Tfidf)\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nfeature_accuracy_checker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nfeature_accuracy_checker' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"RESULT FOR TRIGRAM WITH STOP WORDS (Tfidf)\\n\")\n",
    "feature_result_tgt = nfeature_accuracy_checker(vectorizer=tvec,ngram_range=(1, 3))  # trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Other Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted values: [ 0 -1 -1 ...  1 -1  1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9244812538961618"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "svc = LinearSVC()\n",
    "\n",
    "# tvec.set_params(stop_words = STOP_WORDS, max_features=10000, ngram_range=(1,1))  # unigram -> 0.893\n",
    "tvec.set_params(max_features=10000, ngram_range=(1,3))  # trigram -> 0.8941 accuracy\n",
    "\n",
    "text_clf_svc_tfidf = Pipeline([('vect', tvec),\n",
    "                         ('clf', LinearSVC())])\n",
    "\n",
    "text_clf_svc_tfidf.fit(train.reviews.values, train.Sentiment_encode.astype('int')) \n",
    "predicted_svc = text_clf_svc_tfidf.predict(test.reviews.values)\n",
    "print('predicted values:',predicted_svc)\n",
    "accuracy_score(predicted_svc, test.Sentiment_encode.astype('int'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted values: [ 0  1 -1 ...  1  1  1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8773710927063852"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RidgeClassifier()\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "rc = RidgeClassifier()\n",
    "\n",
    "# tvec.set_params(stop_words = STOP_WORDS, max_features=10000, ngram_range=(1,1))   # unigram: 0.844\n",
    "tvec.set_params(max_features=10000, ngram_range=(1,2)) # bigram: 0.856\n",
    "\n",
    "text_clf_rc_tfidf = Pipeline([('vect', tvec),\n",
    "                         ('clf', RidgeClassifier())])\n",
    "\n",
    "text_clf_rc_tfidf.fit(train.reviews.values, train.Sentiment_encode.astype('int')) \n",
    "predicted_rc = text_clf_rc_tfidf.predict(test.reviews.values)\n",
    "print('predicted values:',predicted_rc)\n",
    "accuracy_score(predicted_rc, test.Sentiment_encode.astype('int'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted values: [ 0 -1 -1 ...  1 -1  1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9279544037759373"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PassiveAggressiveClassifier()\n",
    "\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "pac = PassiveAggressiveClassifier()\n",
    "\n",
    "# tvec.set_params(stop_words = STOP_WORDS, max_features=10000, ngram_range=(1,1))  # unigram: 0.893\n",
    "tvec.set_params(max_features=10000, ngram_range=(1,2))    # bigram: 0.895 (max)\n",
    "\n",
    "text_clf_pac_tfidf = Pipeline([('vect', tvec),\n",
    "                         ('clf', PassiveAggressiveClassifier())])\n",
    "\n",
    "text_clf_pac_tfidf.fit(train.reviews.values, train.Sentiment_encode.astype('int')) \n",
    "predicted_pac = text_clf_pac_tfidf.predict(test.reviews.values)\n",
    "print('predicted values:',predicted_pac)\n",
    "accuracy_score(predicted_pac, test.Sentiment_encode.astype('int'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>结果:</h4> \n",
    "linear_SVC(trigram:0.894) | PassiveAggressiveClassifier(bigram:0.895) | Logistic_regression (unigram: 0.87021) | Ridge Classifier (bigram: 0.856) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Ensemble classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结合上面几个performance比较好的classifier,去建一个ensemble classifier。再看performance是否变好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此function用于比较pipeline classifier, 基于accuracy和training time (TODO：内部结构待弄懂)\n",
    "\n",
    "from time import time\n",
    "\n",
    "def accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n",
    "    t0 = time()\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print (\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "    print (\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    if accuracy > null_accuracy:\n",
    "        print (\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "    elif accuracy == null_accuracy:\n",
    "        print (\"model has the same accuracy with the null accuracy\")\n",
    "    else:\n",
    "        print (\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "    print (\"train and test time: {0:.2f}s\".format(train_test_time))\n",
    "    print (\"-\"*80)\n",
    "    return accuracy, train_test_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result for Logistic Regression\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 87.02%\n",
      "model is 0.81% more accurate than null accuracy\n",
      "train and test time: 2.99s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Linear SVC\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 89.34%\n",
      "model is 3.13% more accurate than null accuracy\n",
      "train and test time: 2.89s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for LinearSVC with L1-based feature selection\n",
      "Pipeline(memory=None,\n",
      "     steps=[('feature_selection', SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
      "     verbose=0),\n",
      "        max_features=None, n...ax_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0))])\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 89.39%\n",
      "model is 3.18% more accurate than null accuracy\n",
      "train and test time: 4.35s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Multinomial NB\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 73.57%\n",
      "model is 12.65% less accurate than null accuracy\n",
      "train and test time: 2.39s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Ridge Classifier\n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=None, solver='auto',\n",
      "        tol=0.001)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 85.67%\n",
      "model is 0.54% less accurate than null accuracy\n",
      "train and test time: 3.75s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Passive-Aggresive\n",
      "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
      "              early_stopping=False, fit_intercept=True, loss='hinge',\n",
      "              max_iter=None, n_iter=None, n_iter_no_change=5, n_jobs=None,\n",
      "              random_state=None, shuffle=True, tol=None,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 89.22%\n",
      "model is 3.00% more accurate than null accuracy\n",
      "train and test time: 2.43s\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Logistic Regression', 0.8702466826965892, 2.990420341491699),\n",
       " ('Linear SVC', 0.8934010152284264, 2.88624906539917),\n",
       " ('LinearSVC with L1-based feature selection',\n",
       "  0.8939353459791611,\n",
       "  4.354011297225952),\n",
       " ('Multinomial NB', 0.735684388636566, 2.3858120441436768),\n",
       " ('Ridge Classifier', 0.8567103036779766, 3.754668951034546),\n",
       " ('Passive-Aggresive', 0.8921542434767121, 2.4337542057037354)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare the accuracy and training time for each classifier\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "names = [\"Logistic Regression\", \"Linear SVC\", \"LinearSVC with L1-based feature selection\",\"Multinomial NB\", \n",
    "         \"Ridge Classifier\", \"Passive-Aggresive\"]\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    LinearSVC(),\n",
    "    Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False))),\n",
    "  ('classification', LinearSVC(penalty=\"l2\"))]),\n",
    "    MultinomialNB(),\n",
    "    RidgeClassifier(),\n",
    "    PassiveAggressiveClassifier()\n",
    "    ]\n",
    "\n",
    "zipped_clf = zip(names,classifiers)\n",
    "tvec = TfidfVectorizer()\n",
    "\n",
    "def classifier_comparator(vectorizer=tvec, n_features=10000, stop_words=None, ngram_range=(1, 1), classifier=zipped_clf):\n",
    "    result = []\n",
    "    vectorizer.set_params(stop_words=STOP_WORDS, max_features=n_features, ngram_range=ngram_range)\n",
    "    for n,c in classifier:\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', c)\n",
    "        ])\n",
    "        print (\"Validation result for {}\".format(n))\n",
    "        print (c)\n",
    "#         clf_accuracy,tt_time = accuracy_summary(checker_pipeline, x_train, y_train, x_validation, y_validation)\n",
    "        clf_accuracy,tt_time = accuracy_summary(checker_pipeline,train.reviews.values, train.Sentiment_encode.astype('int'), test.reviews.values, test.Sentiment_encode.astype('int'))\n",
    "        result.append((n,clf_accuracy,tt_time))\n",
    "    return result\n",
    "\n",
    "bigram_result = classifier_comparator(n_features=10000,ngram_range=(1,2))\n",
    "bigram_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result for Logistic Regression\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 88.90%\n",
      "model is 2.69% more accurate than null accuracy\n",
      "train and test time: 3.32s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Linear SVC\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 92.51%\n",
      "model is 6.30% more accurate than null accuracy\n",
      "train and test time: 3.38s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Multinomial NB\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 74.01%\n",
      "model is 12.20% less accurate than null accuracy\n",
      "train and test time: 2.72s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Ridge Classifier\n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=None, solver='auto',\n",
      "        tol=0.001)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 87.74%\n",
      "model is 1.52% more accurate than null accuracy\n",
      "train and test time: 3.66s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Passive Aggresive Classifier\n",
      "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
      "              early_stopping=False, fit_intercept=True, loss='hinge',\n",
      "              max_iter=None, n_iter=None, n_iter_no_change=5, n_jobs=None,\n",
      "              random_state=None, shuffle=True, tol=None,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 92.64%\n",
      "model is 6.42% more accurate than null accuracy\n",
      "train and test time: 2.94s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Ensemble\n",
      "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)), ('svc', LinearS...=None, shuffle=True, tol=None,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False))],\n",
      "         flatten_transform=None, n_jobs=None, voting='hard', weights=None)\n",
      "null accuracy: 86.21%\n",
      "accuracy score: 90.76%\n",
      "model is 4.55% more accurate than null accuracy\n",
      "train and test time: 4.99s\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# compare results of each classifier and emsemble classifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = LinearSVC()\n",
    "clf3 = MultinomialNB()\n",
    "clf4 = RidgeClassifier()\n",
    "clf5 = PassiveAggressiveClassifier()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('svc', clf2), ('mnb', clf3), ('rcs', clf4), ('pac', clf5)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, eclf], ['Logistic Regression', 'Linear SVC', 'Multinomial NB', 'Ridge Classifier', 'Passive Aggresive Classifier', 'Ensemble']):\n",
    "    checker_pipeline = Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer(max_features=10000,ngram_range=(1, 2))),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "    print (\"Validation result for {}\".format(label))\n",
    "    print (clf)\n",
    "#     clf_accuracy,tt_time = accuracy_summary(checker_pipeline, x_train, y_train, x_validation, y_validation)\n",
    "    clf_accuracy,tt_time = accuracy_summary(checker_pipeline,train.reviews.values, train.Sentiment_encode.astype('int'), test.reviews.values, test.Sentiment_encode.astype('int'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论：LinearSVC and Passive Aggresive classifier yield the best performance ~92% accuracy, even better than the voting classifier(emsemble) of 90% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*P.s: 在此project, 将tutorial中的x_train 改为train.reviews.values，y_train改为 train.Sentiment_encode.astype('int') 即可；<br>\n",
    "test data同理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute positive, negative proportion for each word (tutorial Part 5)\n",
    "# 目的是用于lexical approach for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledSentence(words=['i', 'like', 'eat', 'delicious', 'food', 'that', 's', 'i', 'm', 'cooking', 'food', 'myself', 'case', 'best', 'foods', 'helps', 'lot', 'also', 'best', 'before', 'shelf', 'life'], tags=['all_0']),\n",
       " LabeledSentence(words=['this', 'help', 'eating', 'healthy', 'exercise', 'regular', 'basis'], tags=['all_1']),\n",
       " LabeledSentence(words=['works', 'great', 'especially', 'going', 'grocery', 'store'], tags=['all_3']),\n",
       " LabeledSentence(words=['best', 'idea', 'us'], tags=['all_4']),\n",
       " LabeledSentence(words=['best', 'way'], tags=['all_5'])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "def labelize_tweets_ug(reviews,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, r in zip(reviews.index, reviews):\n",
    "        result.append(LabeledSentence(r.split(), [prefix + '_%s' % i]))\n",
    "    return result\n",
    "\n",
    "all_x_w2v = labelize_tweets_ug(reviews.reviews, 'all')\n",
    "all_x_w2v[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 1. DBOW (Distributed Bag Of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37427/37427 [00:00<00:00, 1135834.04it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2094745.34it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2208593.72it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2428267.60it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1901108.30it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2107484.74it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2172586.20it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2422085.66it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2374638.33it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2273395.26it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2224744.77it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2461816.89it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2216201.71it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2086004.94it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1737083.28it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2446584.73it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2434556.70it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2232178.94it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2234434.78it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2275207.49it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2226069.79it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2176894.49it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2265717.19it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2218299.97it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2223421.32it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2232686.90it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2452470.99it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2260236.65it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2314727.89it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2283514.67it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2311455.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7228604506189331"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm  # Instantly make your loops show a smart progress meter\n",
    "from sklearn import utils\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "model_ug_dbow = Doc2Vec(dm=0, size=100, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dbow.build_vocab([x for x in tqdm(all_x_w2v)])   \n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dbow.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dbow.alpha -= 0.002\n",
    "    model_ug_dbow.min_alpha = model_ug_dbow.alpha\n",
    "    \n",
    "\n",
    "def get_vectors(model, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = model.docvecs[prefix]\n",
    "        n += 1\n",
    "    return vecs\n",
    "\n",
    "train_vecs_dbow = get_vectors(model_ug_dbow, train.reviews, 100)\n",
    "validation_vecs_dbow = get_vectors(model_ug_dbow, test.reviews, 100)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow, train.Sentiment_encode.astype('int'))\n",
    "clf.score(validation_vecs_dbow, test.Sentiment_encode.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained doc2vec model => Q: doc2vec model长什么样子\n",
    "model_ug_dbow.save('d2v_model_ug_dbow.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained doc2vec model can saved and loaded next time: https://radimrehurek.com/gensim/models/doc2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar words of \"bad\":\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('toooo', 0.4004504084587097),\n",
       " ('navagation', 0.39675065875053406),\n",
       " ('toast', 0.388650119304657),\n",
       " ('heavyweight', 0.3769952654838562),\n",
       " ('amd', 0.3658239245414734),\n",
       " ('slot', 0.3447176218032837),\n",
       " ('upset', 0.3436564803123474),\n",
       " ('hysterical', 0.340870201587677),\n",
       " ('dubbed', 0.3226305842399597),\n",
       " ('here', 0.3213273286819458)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('similar words of \"bad\":')\n",
    "model_ug_dbow.most_similar('bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.DMC (Distributed Memory Concatenation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37427/37427 [00:00<00:00, 1659655.93it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2437580.99it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1714075.93it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2353138.40it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2260920.26it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2161785.50it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2196664.23it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2067787.40it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2282684.54it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2201439.05it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2130162.78it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2444755.82it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2094661.49it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2166439.63it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2187450.75it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2343722.90it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2401888.33it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1618202.60it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2405016.18it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2136657.35it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2348140.19it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1969836.57it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2431125.67it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2167067.68it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2155196.68it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2116549.13it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2285243.27it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2344598.02it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2201562.55it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2440119.62it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2411370.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6694273755454626"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_dmc = Doc2Vec(dm=1, dm_concat=1, size=100, window=2, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dmc.build_vocab([x for x in tqdm(all_x_w2v)])  # 唯一差别在 dm_concat\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dmc.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dmc.alpha -= 0.002\n",
    "    model_ug_dmc.min_alpha = model_ug_dmc.alpha\n",
    "    \n",
    "train_vecs_dmc = get_vectors(model_ug_dmc, train.reviews, 100)\n",
    "validation_vecs_dmc = get_vectors(model_ug_dmc, test.reviews, 100)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmc, train.Sentiment_encode.astype('int'))\n",
    "clf.score(validation_vecs_dmc, test.Sentiment_encode.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save doc2vec (dmc) model \n",
    "model_ug_dmc.save('d2v_model_ug_dmc.doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar words of \"good\":\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('great', 0.7447925806045532),\n",
       " ('formative', 0.5763789415359497),\n",
       " ('phenomenal', 0.5657609701156616),\n",
       " ('geat', 0.5610988140106201),\n",
       " ('mask', 0.5583575367927551),\n",
       " ('surrounding', 0.5492602586746216),\n",
       " ('thry', 0.5465722680091858),\n",
       " ('pesima', 0.5336861610412598),\n",
       " ('scratchy', 0.5321255922317505),\n",
       " ('impressive', 0.5299583077430725)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('similar words of \"good\":')\n",
    "model_ug_dmc.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aesthetics', 0.3794088363647461),\n",
       " ('paragraphs', 0.3788681626319885),\n",
       " ('mirroring', 0.368730366230011),\n",
       " ('tongfang', 0.36813265085220337),\n",
       " ('backfired', 0.3655291795730591),\n",
       " ('implied', 0.3621805012226105),\n",
       " ('persistently', 0.36202841997146606),\n",
       " ('cartoon', 0.3591265380382538),\n",
       " ('healthier', 0.35819053649902344),\n",
       " ('coiners', 0.35775208473205566)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ug_dmc.most_similar(positive=['bigger','small'],negative=['big'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3. DMM (Distributed Memory Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37427/37427 [00:00<00:00, 1548555.97it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2169433.61it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2321334.06it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2100547.49it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2140269.62it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2143103.88it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2155196.68it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1639805.45it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1934492.73it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2425940.99it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2307582.40it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2347859.23it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2062543.89it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2143601.37it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2407487.40it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2366833.26it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2152684.56it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2433915.00it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2163155.79it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2070323.59it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2186140.85it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2296980.13it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2338137.53it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2223988.32it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2417274.38it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2106919.03it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2146033.65it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2259683.54it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2461971.33it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2132332.90it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2180068.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7011310000890552"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_dmm = Doc2Vec(dm=1, dm_mean=1, size=100, window=4, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dmm.build_vocab([x for x in tqdm(all_x_w2v)])  #主要差别在 dm_mean\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dmm.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dmm.alpha -= 0.002\n",
    "    model_ug_dmm.min_alpha = model_ug_dmm.alpha\n",
    "    \n",
    "train_vecs_dmm = get_vectors(model_ug_dmm, train.reviews, 100)\n",
    "validation_vecs_dmm = get_vectors(model_ug_dmm, test.reviews, 100)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmm, train.Sentiment_encode.astype('int'))\n",
    "clf.score(validation_vecs_dmm, test.Sentiment_encode.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save doc2vec (dmm) model \n",
    "model_ug_dmm.save('d2v_model_ug_dmm.doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar words of \"bad\":\n",
      "[('good', 0.5860409736633301), ('worst', 0.44234347343444824), ('do', 0.4315769672393799), ('annoying', 0.43012145161628723), ('frustrating', 0.42427948117256165), ('poor', 0.4204840660095215), ('cool', 0.40127480030059814), ('terrible', 0.39951539039611816), ('interesting', 0.39934441447257996), ('disappointed', 0.398501455783844)]\n",
      "------------------------\n",
      "similar words of \"good\":\n",
      "[('great', 0.8083076477050781), ('nice', 0.646178126335144), ('awesome', 0.6101853251457214), ('amazing', 0.6100789308547974), ('bad', 0.5860409736633301), ('excellent', 0.5640246868133545), ('better', 0.5169122219085693), ('love', 0.5033541917800903), ('cool', 0.501633882522583), ('like', 0.49820008873939514)]\n"
     ]
    }
   ],
   "source": [
    "print('similar words of \"bad\":')\n",
    "print(model_ug_dmm.most_similar('bad'))\n",
    "print('------------------------')\n",
    "print('similar words of \"good\":')\n",
    "print(model_ug_dmm.most_similar('good'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析：the system successfully capture similar words of \"good\", yet fails to capture similar words of \"bad\" and it recognizes \"good\" as a similar wods of \"bad\"m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 4. Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74200730252026"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DBOW + DMC\n",
    "def get_concat_vectors(model1,model2, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = np.append(model1.docvecs[prefix],model2.docvecs[prefix])\n",
    "        n += 1\n",
    "    return vecs\n",
    "\n",
    "train_vecs_dbow_dmc = get_concat_vectors(model_ug_dbow,model_ug_dmc, train.reviews, 200)\n",
    "validation_vecs_dbow_dmc = get_concat_vectors(model_ug_dbow,model_ug_dmc, test.reviews, 200)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmc, train.Sentiment_encode.astype('int'))\n",
    "clf.score(validation_vecs_dbow_dmc, test.Sentiment_encode.astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析：Combined method has better performance than DMC: 0.663 and DBOW: 0.723 separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7508237599073827"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DBOW and DMM\n",
    "train_vecs_dbow_dmm = get_concat_vectors(model_ug_dbow,model_ug_dmm, train.reviews, 200)\n",
    "validation_vecs_dbow_dmm = get_concat_vectors(model_ug_dbow,model_ug_dmm, test.reviews, 200)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmm, train.Sentiment_encode.astype('int'))\n",
    "clf.score(validation_vecs_dbow_dmm, test.Sentiment_encode.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Phrase modeling using Gensim </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genism工具 -> 可以extract meaningful bigram\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "tokenized_train = [t.split() for t in train.reviews]\n",
    "phrases = Phrases(tokenized_train)\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['been_using', 'paid_version', 'years_now', 'originally', 'brilliant']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at how bigram works\n",
    "bigram[tokenized_train[0]][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用这个package可以recognize meaning bigram, e.g: paid, version 被当作 paid-vesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledSentence(words=['i', 'like', 'eat', 'delicious', 'food', 'that_s', 'i', 'm', 'cooking', 'food', 'myself', 'case', 'best', 'foods', 'helps_lot', 'also', 'best', 'before', 'shelf', 'life'], tags=['all_0']),\n",
       " LabeledSentence(words=['this', 'help', 'eating', 'healthy', 'exercise', 'regular_basis'], tags=['all_1']),\n",
       " LabeledSentence(words=['works', 'great', 'especially', 'going', 'grocery', 'store'], tags=['all_3']),\n",
       " LabeledSentence(words=['best', 'idea', 'us'], tags=['all_4']),\n",
       " LabeledSentence(words=['best', 'way'], tags=['all_5']),\n",
       " LabeledSentence(words=['amazing'], tags=['all_6']),\n",
       " LabeledSentence(words=['looking_forward', 'app'], tags=['all_8']),\n",
       " LabeledSentence(words=['it', 'helpful', 'site', 'it', 'help', 'foods', 'get'], tags=['all_9']),\n",
       " LabeledSentence(words=['good', 'you'], tags=['all_10']),\n",
       " LabeledSentence(words=['useful_information', 'the', 'amount', 'spelling', 'errors', 'questions', 'validity', 'information', 'shared', 'once', 'fixed', 'stars', 'given'], tags=['all_11'])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tansform corpus using bigram model\n",
    "\n",
    "def labelize_reviews_bg(reviews,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, r in zip(reviews.index, reviews):\n",
    "        result.append(LabeledSentence(bigram[r.split()], [prefix + '_%s' % i]))\n",
    "    return result\n",
    "  \n",
    "# all_x = pd.concat([x_train,x_validation,x_test])\n",
    "all_x_w2v_bg = labelize_reviews_bg(reviews.reviews, 'all')\n",
    "all_x_w2v_bg[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更多meaningful words are indentified, e.g. \"looking_forward\",\"useful_information\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37427/37427 [00:00<00:00, 1363267.18it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2093572.00it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2052699.78it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2084038.71it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1996314.82it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2172225.44it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2229927.64it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2118834.57it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2274778.88it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2112618.31it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1349415.60it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1225307.07it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2110261.14it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2145681.66it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2264050.65it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2102404.22it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2251419.37it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2293054.47it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2118205.58it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2123506.47it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2104292.44it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2182220.53it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2227301.59it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2210179.59it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1447062.33it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2158545.42it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 1413408.51it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2190655.97it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2067433.37it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2149618.85it/s]\n",
      "100%|██████████| 37427/37427 [00:00<00:00, 2141116.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7205450173657494"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DBOW (Distributed Bag Of Words) with bigram detected\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_bg_dbow = Doc2Vec(dm=0, size=100, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_bg_dbow.build_vocab([x for x in tqdm(all_x_w2v_bg)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_bg_dbow.train(utils.shuffle([x for x in tqdm(all_x_w2v_bg)]), total_examples=len(all_x_w2v_bg), epochs=1)\n",
    "    model_bg_dbow.alpha -= 0.002\n",
    "    model_bg_dbow.min_alpha = model_bg_dbow.alpha\n",
    "\n",
    "train_vecs_dbow_bg = get_vectors(model_bg_dbow, train.reviews, 100)\n",
    "validation_vecs_dbow_bg = get_vectors(model_bg_dbow, test.reviews, 100)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_bg, train.Sentiment_encode.astype('int'))\n",
    "clf.score(validation_vecs_dbow_bg, test.Sentiment_encode.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: DMC (Distributed Memory Concatenated) with bigram detected\n",
    "# TODO: DMM (Distributed Memory Mean) with bigram detected\n",
    "# TODO: Combined models\n",
    "# 都是用跟之前同样的方法 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO：trigram model: 方法同上 (只是使用trigram而已)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO：creating joint vectors across different n-grams\n",
    "# 使用performance最好的几个model, e.g:unigram DBOW model .join with. trigram DMM vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Part 8: dimension reduction: Chi2, PCA </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, '$\\\\chi^2$')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chi2 for feature selection （plot the top 20 features）\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tvec = TfidfVectorizer(max_features=10000,ngram_range=(1, 3))  # 这里使用的是tri-gram\n",
    "x_train_tfidf = tvec.fit_transform(train.reviews)\n",
    "x_test_tfidf = tvec.transform(test.reviews)\n",
    "chi2score = chi2(x_train_tfidf, train.Sentiment_encode.astype('int'))[0]  #index 0 -> chi2 statistics of each feature\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "wscores = zip(tvec.get_feature_names(), chi2score)\n",
    "wchi2 = sorted(wscores, key=lambda x:x[1])  # 根据chi2score从小到大sort\n",
    "topchi2 = list(zip(*wchi2[-20:])) # 选top20 -> 倒着选\n",
    "\n",
    "# topchi2[0] #得到 a list of features (tokens)\n",
    "# topchi2[1]  #得到 a list of chi2 scores （对应每个token)\n",
    "\n",
    "x = range(len(topchi2[1])) \n",
    "labels = topchi2[0]\n",
    "plt.barh(x,topchi2[1], align='center', alpha=0.2)  # alpha调节透明度\n",
    "plt.plot(topchi2[1], x, '-o', markersize=5, alpha=0.8)\n",
    "plt.yticks(x, labels)\n",
    "plt.xlabel('$\\chi^2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过chi-2选出来的词, the most useful words有：good, great, worst, etc. 注意这里用reduced-size reviews来分析，总共有5000条。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi2 feature selection evaluation calculated for 1000 features\n",
      "chi2 feature selection evaluation calculated for 2000 features\n",
      "chi2 feature selection evaluation calculated for 3000 features\n",
      "chi2 feature selection evaluation calculated for 4000 features\n",
      "chi2 feature selection evaluation calculated for 5000 features\n",
      "chi2 feature selection evaluation calculated for 6000 features\n",
      "chi2 feature selection evaluation calculated for 7000 features\n",
      "chi2 feature selection evaluation calculated for 8000 features\n",
      "chi2 feature selection evaluation calculated for 9000 features\n"
     ]
    }
   ],
   "source": [
    "# chi2 to reduce dimensions\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "ch2_result = []\n",
    "for n in np.arange(1000,10000,1000):\n",
    "    ch2 = SelectKBest(chi2, k=n)\n",
    "    x_train_chi2_selected = ch2.fit_transform(x_train_tfidf, train.Sentiment_encode.astype('int'))  # 后者只是y_train\n",
    "    x_test_chi2_selected = ch2.transform(x_test_tfidf)  # 注意test data只用transform, 不用fit （防止data leakage)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(x_train_chi2_selected, train.Sentiment_encode.astype('int'))\n",
    "    score = clf.score(x_test_chi2_selected, test.Sentiment_encode.astype('int')) # 后者只是y_test而已\n",
    "    ch2_result.append(score)\n",
    "    print (\"chi2 feature selection evaluation calculated for {} features\".format(n))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Doc2vec (DBOW + DMC) : explained variance of components')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# use PCA to train Doc2vec -> DBOW (Distributed Bag Of Words) \n",
    "scaler = StandardScaler()\n",
    "d2v_dbow_dmc_std = scaler.fit_transform(train_vecs_dbow_dmc)\n",
    "d2v_dbow_dmc_std_val = scaler.fit_transform(validation_vecs_dbow_dmc)\n",
    "d2v_pca = PCA().fit(d2v_dbow_dmc_std)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "x_values = range(1, d2v_pca.n_components_+1)\n",
    "ax.plot(x_values, d2v_pca.explained_variance_ratio_, lw=2, label='explained variance')\n",
    "ax.plot(x_values, np.cumsum(d2v_pca.explained_variance_ratio_), lw=2, label='cumulative explained variance')\n",
    "ax.set_title('Doc2vec (DBOW + DMC) : explained variance of components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: how many gram of DBOW and DMC?  ANSWER: they are both unigram (还没做bigram和trigram的部分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图：蓝线表示每个pc component的解释力(explained_variance)，基本持平；橙线表示随着components增加，累计解释度（cumulative explained variance）基本趋于linear。以上说明不同component之间的解释力差别不大 --> not good (理想情况下是希望找到几个重要component能够有显著的解释度）。说明dimension reduction对这个data set的作用不大，一般PCA对numeric features来reduce dimensions表现较好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Part 9: Neural Networks with Tfidf vectors using Keras </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> ANN with Tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "819/818 [==============================] - 15s 18ms/step - loss: -0.3673 - acc: 0.5231 - val_loss: -1.5356 - val_acc: 0.5934\n",
      "Epoch 2/5\n",
      "819/818 [==============================] - 15s 18ms/step - loss: -2.2094 - acc: 0.6478 - val_loss: -2.1682 - val_acc: 0.6417\n",
      "Epoch 3/5\n",
      "819/818 [==============================] - 14s 17ms/step - loss: -2.7804 - acc: 0.6998 - val_loss: -2.3072 - val_acc: 0.6580\n",
      "Epoch 4/5\n",
      "819/818 [==============================] - 14s 18ms/step - loss: -3.0290 - acc: 0.7239 - val_loss: -2.3646 - val_acc: 0.6709\n",
      "Epoch 5/5\n",
      "819/818 [==============================] - 15s 18ms/step - loss: -3.1763 - acc: 0.7382 - val_loss: -2.3827 - val_acc: 0.6794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a5b313e10>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_generator(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].toarray()\n",
    "        y_batch = y_data[y_data.index[index_batch]]\n",
    "        counter += 1\n",
    "        yield X_batch,y_batch\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0\n",
    "            \n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=10000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(generator=batch_generator(x_train_tfidf, train.Sentiment_encode.astype('int'), 32),\n",
    "                    epochs=5, validation_data=(x_test_tfidf, test.Sentiment_encode.astype('int')),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "again 注意命名 -> y_train: train.Sentiment_encode.astype('int') ; y_test: test.Sentiment_encode.astype('int')\n",
    "x_train: train.reviews; x_test: test.reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在5th epoch之后，validation accuracy达到最高值为0.679;显著小于logistic regression的validation accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "819/818 [==============================] - 16s 20ms/step - loss: -0.3441 - acc: 0.5351 - val_loss: -1.5177 - val_acc: 0.5924\n",
      "Epoch 2/5\n",
      "819/818 [==============================] - 16s 19ms/step - loss: -2.2079 - acc: 0.6474 - val_loss: -2.1684 - val_acc: 0.6417\n",
      "Epoch 3/5\n",
      "819/818 [==============================] - 16s 20ms/step - loss: -2.7811 - acc: 0.6994 - val_loss: -2.3088 - val_acc: 0.6580\n",
      "Epoch 4/5\n",
      "819/818 [==============================] - 15s 18ms/step - loss: -3.0308 - acc: 0.7232 - val_loss: -2.3637 - val_acc: 0.6709\n",
      "Epoch 5/5\n",
      "819/818 [==============================] - 14s 17ms/step - loss: -3.1758 - acc: 0.7383 - val_loss: -2.3830 - val_acc: 0.6786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a7548b4a8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try normalize input to see if yield better results\n",
    "from sklearn.preprocessing import Normalizer\n",
    "norm = Normalizer().fit(x_train_tfidf)\n",
    "x_train_tfidf_norm = norm.transform(x_train_tfidf)\n",
    "x_test_tfidf_norm = norm.transform(x_test_tfidf)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=10000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(generator=batch_generator(x_train_tfidf_norm, train.Sentiment_encode.astype('int'), 32),\n",
    "                    epochs=5, validation_data=(x_test_tfidf_norm, test.Sentiment_encode.astype('int')),\n",
    "                    steps_per_epoch=x_train_tfidf_norm.shape[0]/32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize之后, validation accuracy反而下降了... 其实tf-idf vectorizer()过程中input就已经normalize了，因此之后无需再normalize一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "819/818 [==============================] - 15s 18ms/step - loss: -0.2704 - acc: 0.5131 - val_loss: -1.3914 - val_acc: 0.5807\n",
      "Epoch 2/5\n",
      "819/818 [==============================] - 13s 16ms/step - loss: -2.0752 - acc: 0.6367 - val_loss: -2.1224 - val_acc: 0.6378\n",
      "Epoch 3/5\n",
      "819/818 [==============================] - 13s 16ms/step - loss: -2.6875 - acc: 0.6915 - val_loss: -2.2939 - val_acc: 0.6556\n",
      "Epoch 4/5\n",
      "819/818 [==============================] - 13s 16ms/step - loss: -2.9531 - acc: 0.7148 - val_loss: -2.3672 - val_acc: 0.6659\n",
      "Epoch 5/5\n",
      "819/818 [==============================] - 13s 16ms/step - loss: -3.1071 - acc: 0.7302 - val_loss: -2.3922 - val_acc: 0.6763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a55689278>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try add dropout layer to avoid overfitting to the training data\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(64, activation='relu', input_dim=10000))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.fit_generator(generator=batch_generator(x_train_tfidf, train.Sentiment_encode.astype('int'), 32),\n",
    "                    epochs=5, validation_data=(x_test_tfidf, test.Sentiment_encode.astype('int')),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "even adding dropout layer, the neural network model still underperform the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "819/818 [==============================] - 14s 18ms/step - loss: -0.3425 - acc: 0.5174 - val_loss: -1.5247 - val_acc: 0.5812\n",
      "Epoch 2/5\n",
      "819/818 [==============================] - 13s 15ms/step - loss: -2.2055 - acc: 0.6459 - val_loss: -2.1672 - val_acc: 0.6418\n",
      "Epoch 3/5\n",
      "819/818 [==============================] - 13s 16ms/step - loss: -2.7791 - acc: 0.6991 - val_loss: -2.3133 - val_acc: 0.6620\n",
      "Epoch 4/5\n",
      "819/818 [==============================] - 13s 16ms/step - loss: -3.0303 - acc: 0.7227 - val_loss: -2.3687 - val_acc: 0.6699\n",
      "Epoch 5/5\n",
      "819/818 [==============================] - 13s 15ms/step - loss: -3.1773 - acc: 0.7378 - val_loss: -2.3832 - val_acc: 0.6791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a5751add8>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffling data --> another way to avoid overfitting\n",
    "\n",
    "def batch_generator_shuffle(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    np.random.shuffle(index)\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].toarray()\n",
    "        y_batch = y_data[y_data.index[index_batch]]\n",
    "        counter += 1\n",
    "        yield X_batch,y_batch\n",
    "        if (counter > number_of_batches):\n",
    "            np.random.shuffle(index)\n",
    "            counter=0\n",
    "\n",
    "            \n",
    "model_s = Sequential()\n",
    "model_s.add(Dense(64, activation='relu', input_dim=10000))\n",
    "model_s.add(Dense(1, activation='sigmoid'))\n",
    "model_s.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_s.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, train.Sentiment_encode.astype('int'), 32),\n",
    "                    epochs=5, validation_data=(x_test_tfidf, test.Sentiment_encode.astype('int')),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表现都差不多，仍然underperform logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Adjust Learning rate </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "819/818 [==============================] - 15s 19ms/step - loss: -1.4486 - acc: 0.6049 - val_loss: -2.1923 - val_acc: 0.6515\n",
      "Epoch 2/2\n",
      "819/818 [==============================] - 14s 18ms/step - loss: -2.8036 - acc: 0.7104 - val_loss: -2.2674 - val_acc: 0.6751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a588adf98>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# goal: to improve the performance of a neural net can be to adjust the learning rate\n",
    "import keras\n",
    "\n",
    "# Learning rate (lr) = 0.005\n",
    "custom_adam = keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model_testing_2 = Sequential()\n",
    "model_testing_2.add(Dense(64, activation='relu', input_dim=10000))\n",
    "model_testing_2.add(Dense(1, activation='sigmoid'))\n",
    "model_testing_2.compile(optimizer=custom_adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_testing_2.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, train.Sentiment_encode.astype('int'), 32),\n",
    "                    epochs=2, validation_data=(x_test_tfidf, test.Sentiment_encode.astype('int')),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "819/818 [==============================] - 16s 19ms/step - loss: 0.1760 - acc: 0.5025 - val_loss: -0.4058 - val_acc: 0.5652\n",
      "Epoch 2/2\n",
      "819/818 [==============================] - 15s 18ms/step - loss: -1.1442 - acc: 0.5876 - val_loss: -1.5639 - val_acc: 0.5904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a59248400>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try the same for the following learning rate:\n",
    "# Learning rate (lr) = 0.005\n",
    "# Learning rate (lr) = 0.01\n",
    "# Learning rate (lr) = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try adjust learning rate to (0.0005, 0.005, 0.01, 0.1) -> none of it outperform the default learning rate of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Increasing Number of Nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "819/818 [==============================] - 25s 31ms/step - loss: -0.6922 - acc: 0.5645 - val_loss: -1.9138 - val_acc: 0.6169\n",
      "Epoch 2/2\n",
      "819/818 [==============================] - 26s 31ms/step - loss: -2.5027 - acc: 0.6757 - val_loss: -2.2969 - val_acc: 0.6636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a5a1ab748>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# goal is to improve performance\n",
    "\n",
    "model_s_2 = Sequential()\n",
    "model_s_2.add(Dense(128, activation='relu', input_dim=10000))\n",
    "model_s_2.add(Dense(1, activation='sigmoid'))\n",
    "model_s_2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_s_2.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, train.Sentiment_encode.astype('int'), 32),\n",
    "                    epochs=2, validation_data=(x_test_tfidf, test.Sentiment_encode.astype('int')),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use 128 hidden nodes --> 即便如此，仍然underperform 一般的logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：As a result, in this particular case, neural network models failed to outperform logistic regression. This might be due to the high dimensionality and sparse characteristics of the textual data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Part 10: implement a neural network with Doc2Vec vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previous practice with Neural Network, we found that with high-dimensional sparse data, neural network did not perform well. In this post, I will see if feeding document vectors from Doc2Vec models or word vectors is any different from Tf-idf vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7604778990762654\n",
      "0.754653130287648\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "\n",
    "def get_concat_vectors(model1,model2, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = np.append(model1.docvecs[prefix],model2.docvecs[prefix])\n",
    "        n += 1\n",
    "    return vecs\n",
    "\n",
    "# load pre-trained doc2vec model and prepare\n",
    "model_ug_dbow = Doc2Vec.load('d2v_model_ug_dbow.doc2vec')   # unigram\n",
    "model_ug_dmm = Doc2Vec.load('d2v_model_ug_dmm.doc2vec')    # unigram\n",
    "\n",
    "model_ug_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_ug_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "# train combined model \n",
    "train_vecs_ugdbow_ugdmm = get_concat_vectors(model_ug_dbow,model_ug_dmm, train.reviews, 200)\n",
    "validation_vecs_ugdbow_ugdmm = get_concat_vectors(model_ug_dbow,model_ug_dmm, test.reviews, 200)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_ugdbow_ugdmm, train.Sentiment_encode.astype('int'))\n",
    "\n",
    "print (clf.score(train_vecs_ugdbow_ugdmm, train.Sentiment_encode.astype('int')))  # get training accuracy\n",
    "print (clf.score(validation_vecs_ugdbow_ugdmm, test.Sentiment_encode.astype('int')))   # get test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，跟tutorial (https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-10-neural-network-with-a6441269aa3c) 不太一样，这里我使用的doc2vec model都是unigram model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用neural network 去trained doc2vec model得到的training accuracy 和 test accuracy 分别为0.76, 0.754.要比前一章用neural network去train sparse matrix结果要好很多。但accuracy还是低于使用简单方法ti-idf + logistic regression去得到的accuracy结果（0.88）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26198 samples, validate on 11229 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: -2.7605e-01 - acc: 0.5153 - val_loss: -6.7480e-01 - val_acc: 0.5436\n",
      "Epoch 2/10\n",
      " - 1s - loss: -7.3707e-01 - acc: 0.5527 - val_loss: -8.6068e-01 - val_acc: 0.5506\n",
      "Epoch 3/10\n",
      " - 2s - loss: -9.2326e-01 - acc: 0.5691 - val_loss: -9.4030e-01 - val_acc: 0.5809\n",
      "Epoch 4/10\n",
      " - 1s - loss: -1.0622e+00 - acc: 0.5827 - val_loss: -1.0206e+00 - val_acc: 0.5729\n",
      "Epoch 5/10\n",
      " - 2s - loss: -1.1871e+00 - acc: 0.5901 - val_loss: -1.0516e+00 - val_acc: 0.5913\n",
      "Epoch 6/10\n",
      " - 1s - loss: -1.2879e+00 - acc: 0.5979 - val_loss: -1.1026e+00 - val_acc: 0.5800\n",
      "Epoch 7/10\n",
      " - 1s - loss: -1.3961e+00 - acc: 0.6036 - val_loss: -1.1326e+00 - val_acc: 0.5800\n",
      "Epoch 8/10\n",
      " - 1s - loss: -1.4999e+00 - acc: 0.6118 - val_loss: -1.1618e+00 - val_acc: 0.5684\n",
      "Epoch 9/10\n",
      " - 1s - loss: -1.6046e+00 - acc: 0.6149 - val_loss: -1.1446e+00 - val_acc: 0.5970\n",
      "Epoch 10/10\n",
      " - 2s - loss: -1.6993e+00 - acc: 0.6226 - val_loss: -1.1910e+00 - val_acc: 0.5874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a37f48518>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try different data structure\n",
    "import numpy as np\n",
    "seed = 7\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Model example 1: 1 hidden layer with 64 hidden nodes\n",
    "np.random.seed(seed)  \n",
    "model_d2v_01 = Sequential()\n",
    "model_d2v_01.add(Dense(64, activation='relu', input_dim=200))\n",
    "model_d2v_01.add(Dense(1, activation='sigmoid'))\n",
    "model_d2v_01.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_d2v_01.fit(train_vecs_ugdbow_ugdmm, train.Sentiment_encode.astype('int'),\n",
    "                 validation_data=(validation_vecs_ugdbow_ugdmm, test.Sentiment_encode.astype('int')),\n",
    "                 epochs=10, batch_size=32, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26198 samples, validate on 11229 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: -4.7789e-01 - acc: 0.5356 - val_loss: -9.2754e-01 - val_acc: 0.5635\n",
      "Epoch 2/10\n",
      " - 2s - loss: -1.0303e+00 - acc: 0.5862 - val_loss: -1.1085e+00 - val_acc: 0.6050\n",
      "Epoch 3/10\n",
      " - 2s - loss: -1.2358e+00 - acc: 0.6047 - val_loss: -1.1763e+00 - val_acc: 0.5767\n",
      "Epoch 4/10\n",
      " - 2s - loss: -1.4189e+00 - acc: 0.6167 - val_loss: -1.2573e+00 - val_acc: 0.6139\n",
      "Epoch 5/10\n",
      " - 2s - loss: -1.5961e+00 - acc: 0.6280 - val_loss: -1.2786e+00 - val_acc: 0.5982\n",
      "Epoch 6/10\n",
      " - 2s - loss: -1.7660e+00 - acc: 0.6346 - val_loss: -1.2987e+00 - val_acc: 0.6203\n",
      "Epoch 7/10\n",
      " - 2s - loss: -1.9087e+00 - acc: 0.6450 - val_loss: -1.3136e+00 - val_acc: 0.6214\n",
      "Epoch 8/10\n",
      " - 2s - loss: -2.0434e+00 - acc: 0.6551 - val_loss: -1.3217e+00 - val_acc: 0.6166\n",
      "Epoch 9/10\n",
      " - 2s - loss: -2.1433e+00 - acc: 0.6606 - val_loss: -1.3240e+00 - val_acc: 0.6279\n",
      "Epoch 10/10\n",
      " - 2s - loss: -2.2528e+00 - acc: 0.6681 - val_loss: -1.2916e+00 - val_acc: 0.6294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2c3a90f0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model example 2: 2 hidden layers with 64 hidden nodes each\n",
    "np.random.seed(seed)\n",
    "model_d2v_02 = Sequential()\n",
    "model_d2v_02.add(Dense(64, activation='relu', input_dim=200))\n",
    "model_d2v_02.add(Dense(64, activation='relu'))  # difference to model 1\n",
    "model_d2v_02.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_d2v_02.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_d2v_02.fit(train_vecs_ugdbow_ugdmm, train.Sentiment_encode.astype('int'),\n",
    "                 validation_data=(validation_vecs_ugdbow_ugdmm, test.Sentiment_encode.astype('int')),\n",
    "                 epochs=10, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model2相比model1增加了hidden layers nodes，accuracy也从0.5+ 上升到0.6+ <br>\n",
    "可以尝试不同的neural network settings: (去比较不同的accuracy结果）\n",
    "- a range of hidden layers (from 1 to 3) \n",
    "- a range of hidden nodes for each hidden layer (64, 128, 256, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26198 samples, validate on 11229 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: -6.8111e-01 - acc: 0.5596 - val_loss: -1.0225e+00 - val_acc: 0.5676\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.56764, saving model to d2v_09_best_weights.01-0.5676.hdf5\n",
      "Epoch 2/100\n",
      " - 4s - loss: -1.1008e+00 - acc: 0.5951 - val_loss: -1.0847e+00 - val_acc: 0.5993\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.56764 to 0.59934, saving model to d2v_09_best_weights.02-0.5993.hdf5\n",
      "Epoch 3/100\n",
      " - 4s - loss: -1.3470e+00 - acc: 0.6168 - val_loss: -1.2156e+00 - val_acc: 0.5974\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.59934\n",
      "Epoch 4/100\n",
      " - 4s - loss: -1.5927e+00 - acc: 0.6351 - val_loss: -1.2968e+00 - val_acc: 0.6057\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.59934 to 0.60566, saving model to d2v_09_best_weights.04-0.6057.hdf5\n",
      "Epoch 5/100\n",
      " - 4s - loss: -1.8233e+00 - acc: 0.6543 - val_loss: -1.3134e+00 - val_acc: 0.6427\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.60566 to 0.64271, saving model to d2v_09_best_weights.05-0.6427.hdf5\n",
      "Epoch 6/100\n",
      " - 4s - loss: -2.0261e+00 - acc: 0.6668 - val_loss: -1.3799e+00 - val_acc: 0.6423\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.64271\n",
      "Epoch 7/100\n",
      " - 4s - loss: -2.1777e+00 - acc: 0.6742 - val_loss: -1.4180e+00 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.64271\n",
      "Epoch 8/100\n",
      " - 4s - loss: -2.3502e+00 - acc: 0.6842 - val_loss: -1.4235e+00 - val_acc: 0.6284\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.64271\n",
      "Epoch 9/100\n",
      " - 4s - loss: -2.4754e+00 - acc: 0.6922 - val_loss: -1.4716e+00 - val_acc: 0.6531\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.64271 to 0.65313, saving model to d2v_09_best_weights.09-0.6531.hdf5\n",
      "Epoch 10/100\n",
      " - 4s - loss: -2.5665e+00 - acc: 0.6955 - val_loss: -1.3834e+00 - val_acc: 0.6391\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.65313\n",
      "Epoch 11/100\n",
      " - 4s - loss: -2.6724e+00 - acc: 0.7051 - val_loss: -1.3613e+00 - val_acc: 0.6532\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.65313 to 0.65322, saving model to d2v_09_best_weights.11-0.6532.hdf5\n",
      "Epoch 12/100\n",
      " - 4s - loss: -2.7369e+00 - acc: 0.7061 - val_loss: -1.4090e+00 - val_acc: 0.6366\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.65322\n",
      "Epoch 13/100\n",
      " - 4s - loss: -2.7983e+00 - acc: 0.7122 - val_loss: -1.3421e+00 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.65322\n",
      "Epoch 14/100\n",
      " - 4s - loss: -2.8375e+00 - acc: 0.7147 - val_loss: -1.3691e+00 - val_acc: 0.6515\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.65322\n",
      "Epoch 15/100\n",
      " - 4s - loss: -2.9024e+00 - acc: 0.7185 - val_loss: -1.3556e+00 - val_acc: 0.6553\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.65322 to 0.65527, saving model to d2v_09_best_weights.15-0.6553.hdf5\n",
      "Epoch 16/100\n",
      " - 4s - loss: -2.9251e+00 - acc: 0.7225 - val_loss: -1.3815e+00 - val_acc: 0.6464\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.65527\n",
      "Epoch 17/100\n",
      " - 4s - loss: -2.9482e+00 - acc: 0.7226 - val_loss: -1.3529e+00 - val_acc: 0.6619\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.65527 to 0.66186, saving model to d2v_09_best_weights.17-0.6619.hdf5\n",
      "Epoch 18/100\n",
      " - 4s - loss: -2.9783e+00 - acc: 0.7269 - val_loss: -1.3305e+00 - val_acc: 0.6616\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.66186\n",
      "Epoch 19/100\n",
      " - 4s - loss: -3.0044e+00 - acc: 0.7285 - val_loss: -1.3215e+00 - val_acc: 0.6585\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.66186\n",
      "Epoch 20/100\n",
      " - 3s - loss: -3.0720e+00 - acc: 0.7341 - val_loss: -1.2633e+00 - val_acc: 0.6531\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.66186\n",
      "Epoch 21/100\n",
      " - 4s - loss: -3.0821e+00 - acc: 0.7357 - val_loss: -1.1402e+00 - val_acc: 0.6619\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.66186\n",
      "Epoch 22/100\n",
      " - 3s - loss: -3.0917e+00 - acc: 0.7360 - val_loss: -1.2331e+00 - val_acc: 0.6607\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.66186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a68327d68>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use \"check point\" function of keras can be more efficient\n",
    "# used “checkpoint” and “earlystop”. You can set the “checkpoint” function with options, \n",
    "# and with the below parameter setting, “checkpoint” will save the best performing model up until the point of running\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "filepath=\"d2v_09_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_acc', patience=5, mode='max') \n",
    "callbacks_list = [checkpoint, early_stop]\n",
    "np.random.seed(seed)\n",
    "\n",
    "model_d2v_09_es = Sequential()\n",
    "model_d2v_09_es.add(Dense(256, activation='relu', input_dim=200))\n",
    "model_d2v_09_es.add(Dense(256, activation='relu'))\n",
    "model_d2v_09_es.add(Dense(256, activation='relu'))\n",
    "model_d2v_09_es.add(Dense(1, activation='sigmoid'))\n",
    "model_d2v_09_es.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_d2v_09_es.fit(train_vecs_ugdbow_ugdmm, train.Sentiment_encode.astype('int'),\n",
    "                    validation_data=(validation_vecs_ugdbow_ugdmm, test.Sentiment_encode.astype('int')), \n",
    "                    epochs=100, batch_size=32, verbose=2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the best performance of the model is 0.66 (accuracy), and not getting any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11229/11229 [==============================] - 1s 58us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-1.233111369980338, 0.6606999732781543]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if evaluate the model, it will show the results from last best performance\n",
    "model_d2v_09_es.evaluate(x=validation_vecs_ugdbow_ugdmm, y=test.Sentiment_encode.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11229/11229 [==============================] - 1s 70us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-1.3528976420471235, 0.6618576898861679]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "loaded_model = load_model('d2v_09_best_weights.17-0.6619.hdf5')\n",
    "loaded_model.evaluate(x=validation_vecs_ugdbow_ugdmm, y=test.Sentiment_encode.astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果相比tf-idf vectorizer()+logistic regression还是差很多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Implement a neural network with Word2Vec vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将doc2vec转化成word2vec，再用neural network去train word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7261554902484638"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approach1: averaged word vectors\n",
    "model_ug_dmm = Doc2Vec.load('d2v_model_ug_dmm.doc2vec')\n",
    "model_ug_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "def get_w2v_ugdbowdmm(tweet, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += np.append(model_ug_dbow[word],model_ug_dmm[word]).reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "train_vecs_w2v_dbowdmm = np.concatenate([get_w2v_ugdbowdmm(z, 200) for z in train.reviews])\n",
    "validation_vecs_w2v_dbowdmm = np.concatenate([get_w2v_ugdbowdmm(z, 200) for z in test.reviews])\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_dbowdmm, train.Sentiment_encode.astype('int'))\n",
    "clf.score(validation_vecs_w2v_dbowdmm, test.Sentiment_encode.astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于averaged word vectors的unigram model得到的accuracy rate(0.72) 要比doc2vec好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7514471457832398"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scale input vector --> 减少processing time\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "train_vecs_w2v_dbowdmm_s = scale(train_vecs_w2v_dbowdmm)\n",
    "validation_vecs_w2v_dbowdmm_s = scale(validation_vecs_w2v_dbowdmm)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_dbowdmm_s, train.Sentiment_encode.astype('int'))\n",
    "clf.score(validation_vecs_w2v_dbowdmm_s, test.Sentiment_encode.astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processing time 减少，同时accuracy也提高了(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7345266720099741"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approach2: summation word vector\n",
    "# def get_w2v_ugdbowdmm_sum: ... 定义function (可参考之前的average function)\n",
    "\n",
    "def get_w2v_ugdbowdmm_sum(tweet, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += np.append(model_ug_dbow[word],model_ug_dmm[word]).reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return vec\n",
    "\n",
    "train_vecs_w2v_dbowdmm_sum = np.concatenate([get_w2v_ugdbowdmm_sum(z, 200) for z in train.reviews])\n",
    "validation_vecs_w2v_dbowdmm_sum = np.concatenate([get_w2v_ugdbowdmm_sum(z, 200) for z in test.reviews])\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_dbowdmm_sum, train.Sentiment_encode.astype('int'))\n",
    "clf.score(validation_vecs_w2v_dbowdmm_sum, test.Sentiment_encode.astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用sum word vector相比averaged word vector, accuracy减少 （0.73）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7210793481164841\n",
      "0.6998842283373408\n"
     ]
    }
   ],
   "source": [
    "# approach3: Word vectors extracted from Doc2Vec models with TFIDF weighting (Average/Sum) => training时间长\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tvec = TfidfVectorizer(min_df=2)\n",
    "tvec.fit_transform(train.reviews)\n",
    "tfidf = dict(zip(tvec.get_feature_names(), tvec.idf_))\n",
    "\n",
    "def get_w2v_general(tweet, size, vectors, aggregation='mean'):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += vectors[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if aggregation == 'mean':\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "    elif aggregation == 'sum':\n",
    "        return vec\n",
    "\n",
    "# TODO： figure out what does it mean here\n",
    "w2v_tfidf = {}\n",
    "for w in model_ug_dbow.wv.vocab.keys():\n",
    "    if w in tvec.get_feature_names():\n",
    "        w2v_tfidf[w] = np.append(model_ug_dbow[w],model_ug_dmm[w]) * tfidf[w]\n",
    "\n",
    "# average\n",
    "train_vecs_w2v_tfidf_mean = scale(np.concatenate([get_w2v_general(z, 200, w2v_tfidf, 'mean') for z in train.reviews]))\n",
    "validation_vecs_w2v_tfidf_mean = scale(np.concatenate([get_w2v_general(z, 200, w2v_tfidf, 'mean') for z in test.reviews]))\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_tfidf_mean, train.Sentiment_encode.astype('int'))\n",
    "print (clf.score(validation_vecs_w2v_tfidf_mean, test.Sentiment_encode.astype('int')))\n",
    "\n",
    "# sum\n",
    "train_vecs_w2v_tfidf_sum = scale(np.concatenate([get_w2v_general(z, 200, w2v_tfidf, 'sum') for z in train.reviews]))\n",
    "validation_vecs_w2v_tfidf_sum = scale(np.concatenate([get_w2v_general(z, 200, w2v_tfidf, 'sum') for z in test.reviews]))\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_tfidf_sum, train.Sentiment_encode.astype('int'))\n",
    "print (clf.score(validation_vecs_w2v_tfidf_sum, test.Sentiment_encode.astype('int')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average word vectors的accuracy高于sum word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_ug_cbow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-229cef42f208>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mw2v_pos_hmean_01\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_ug_cbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_hmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mw2v_pos_hmean_01\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ug_cbow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_ug_sg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpos_hmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_ug_cbow' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Word vectors extracted from Doc2Vec models with custom weighting (Average/Sum)\n",
    "# TODO: Word vectors extracted from pre-trained GloVe (Average/Sum)\n",
    "# TODO: Word vectors extracted from pre-trained Google News Word2Vec (Average/Sum)\n",
    "\n",
    "# TODO: Separately trained Word2Vec (Average/Sum)\n",
    "\n",
    "w2v_pos_hmean_01 = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    if w in pos_hmean.keys():\n",
    "        w2v_pos_hmean_01[w] = np.append(model_ug_cbow[w],model_ug_sg[w]) * pos_hmean[w]\n",
    "        \n",
    "train_vecs_w2v_poshmean_mean_01 = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean_01, 'mean') for z in train.reviews]))\n",
    "validation_vecs_w2v_poshmean_mean_01 = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean_01, 'mean') for z in test.reviews]))\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_poshmean_mean_01, train.Sentiment_encode.astype('int'))\n",
    "print (clf.score(validation_vecs_w2v_poshmean_mean_01, test.Sentiment_encode.astype('int')))\n",
    "\n",
    "train_vecs_w2v_poshmean_sum_01 = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean_01, 'sum') for z in train.reviews]))\n",
    "validation_vecs_w2v_poshmean_sum_01 = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean_01, 'sum') for z in test.reviews]))\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_poshmean_sum_01, train.Sentiment_encode.astype('int'))\n",
    "print (clf.score(validation_vecs_w2v_poshmean_sum_01, test.Sentiment_encode.astype('int')))\n",
    "\n",
    "# TODO: Separately trained Word2Vec with custom weighting (Average/Sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_vecs_w2v_poshmean_mean_01' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-6343bd2974bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# TODO:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_w2v_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_vecs_w2v_poshmean_mean_01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mvalidation_w2v_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_vecs_w2v_poshmean_mean_01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_vecs_w2v_poshmean_mean_01' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Neural Network with Word2Vec\n",
    "\n",
    "# TODO:\n",
    "train_w2v_final = train_vecs_w2v_poshmean_mean_01\n",
    "validation_w2v_final = validation_vecs_w2v_poshmean_mean_01\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "filepath=\"w2v_01_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_acc', patience=5, mode='max') \n",
    "callbacks_list = [checkpoint, early_stop]\n",
    "np.random.seed(seed)\n",
    "\n",
    "model_w2v_01 = Sequential()\n",
    "model_w2v_01.add(Dense(256, activation='relu', input_dim=200))\n",
    "model_w2v_01.add(Dense(256, activation='relu'))\n",
    "model_w2v_01.add(Dense(256, activation='relu'))\n",
    "model_w2v_01.add(Dense(1, activation='sigmoid'))\n",
    "model_w2v_01.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_w2v_01.fit(train_w2v_final, y_train, validation_data=(validation_w2v_final, y_validation), \n",
    "                 epochs=100, batch_size=32, verbose=2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deeping Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Representation: Sequence Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "## Tokenize the sentences\n",
    "\n",
    "max_features = 276    # max length of reviews\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "# tokenizer.fit_on_texts(list(train_X)+list(test_X))\n",
    "tokenizer.fit_on_texts(list(X_train)+list(X_test))\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad Sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 276\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Embedding Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will be using GLoVE Word2Vec embeddings to explain the enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load glove index\n",
    "\n",
    "def load_glove_index():\n",
    "#     EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    EMBEDDING_FILE = './input/glove.42B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]    # 选最常出现的300 words\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    return embeddings_index\n",
    "\n",
    "glove_embedding_index = load_glove_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(glove_embedding_index.items())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create glove (add polarity and lowercase as well)\n",
    "\n",
    "def create_glove(word_index,embeddings_index):\n",
    "    emb_mean,emb_std = -0.005838499,0.48782197\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    embed_size = all_embs.shape[1]\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size+4))\n",
    "    \n",
    "    count_found = nb_words\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        word_sent = TextBlob(word).sentiment\n",
    "        # Extra information we are passing to our embeddings\n",
    "        extra_embed = [word_sent.polarity,word_sent.subjectivity]\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] =  np.append(embedding_vector,extra_embed)\n",
    "        else:\n",
    "            if word.islower():\n",
    "                embedding_vector = embeddings_index.get(word.capitalize())\n",
    "                if embedding_vector is not None: \n",
    "                    embedding_matrix[i] = np.append(embedding_vector,extra_embed)\n",
    "                else:\n",
    "                    embedding_matrix[i,300:] = extra_embed\n",
    "                    count_found-=1\n",
    "            else:\n",
    "                embedding_matrix[i,300:] = extra_embed\n",
    "                count_found-=1\n",
    "    print(\"Got embedding for \",count_found,\" words.\")\n",
    "    return embedding_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
